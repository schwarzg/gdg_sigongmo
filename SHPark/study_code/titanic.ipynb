{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"titanic.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"slBvnN_s6kT9","colab_type":"code","outputId":"8846f53c-acce-40f5-8bd0-f0866a7828b9","executionInfo":{"status":"ok","timestamp":1549500488204,"user_tz":-540,"elapsed":34378,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"I8NqNwuP8EcW","colab_type":"code","colab":{}},"cell_type":"code","source":["### 데이터 처리\n","\n","\n","# 데이터 읽기\n","import pandas as pd\n","\n","read_train=pd.read_csv('/content/gdrive/My Drive/train.csv')\n","read_test = pd.read_csv('/content/gdrive/My Drive/test.csv')\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GajM8Br4GjPH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"79b0026c-a9e8-4b64-d3c9-67e97a970d2c","executionInfo":{"status":"ok","timestamp":1549512030947,"user_tz":-540,"elapsed":692,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}}},"cell_type":"code","source":[""],"execution_count":138,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"}]},{"metadata":{"id":"kTB-55LKFPNz","colab_type":"code","outputId":"af352424-b065-4445-a954-c23d42f494b9","executionInfo":{"status":"ok","timestamp":1549519367584,"user_tz":-540,"elapsed":701,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"cell_type":"code","source":["data_train=read_train #[[\"Survived\",\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]] # data categories\n","data_test = read_test #[[\"PassengerId\",\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]]\n","\n","### 결측값 채워넣기\n","import numpy as np\n","# 대체되는 값에 따라서 성능이 좌지우지될 수 있기에 신중하게!!\n","\n","# 이름으로부터 성별 추측\n","data_train['Initial']= data_train.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n","data_test['Initial']= data_test.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n","\n","# 결측값이 있는 데이터에 대해서 임의적으로 값을 집어넣어주어야 함\n","#data_train=data_train.dropna(how=\"any\") # dropna: 결측값(NaN)있는 행을 제거, how(\"any\"): 결측값이 하나라도 있을 경우 drop, how(\"all\"): 모든 값이 결측값일 경우 drop \n","\n","\n","\n","### 결측값 채워넣기\n","import numpy as np\n","# 대체되는 값에 따라서 성능이 좌지우지될 수 있기에 신중하게!!\n","\n","# 이름으로부터 성별 추측\n","data_train['Initial']= read_train.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n","data_test['Initial']= read_test.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n","\n","#print(data_train.head())\n","#print(data_train.head())\n","#pd.crosstab(data_train['Initial'], data_train['Sex']).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex \n","\n","# 성별을 알맞게 바꿔넣기\n","data_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n","                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)\n","\n","data_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n","                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)\n","\n","\n","\n","#print('train 데이터에의 성별에 값에 대한 평균')\n","#print(data_train.groupby('Initial').mean()) # 남성보다는 여성의 생존율이 더 높음을 볼 수 있음\n","# 그룹별\n","grouped_train = data_train.groupby(['Sex','Pclass', 'Age'])\n","data_train.Age = grouped_train.Age.apply(lambda x: x.fillna(x.median()))\n","\n","grouped_test = data_test.groupby(['Sex','Pclass', 'Age'])\n","data_test.Age = grouped_test.Age.apply(lambda x: x.fillna(x.median()))\n","\n","# # 나이 채워넣기\n","# data_train.loc[(data_train.Age.isnull())&(data_train.Initial=='Mr'),'Age'] = 33\n","# data_train.loc[(data_train.Age.isnull())&(data_train.Initial=='Mrs'),'Age'] = 36\n","# data_train.loc[(data_train.Age.isnull())&(data_train.Initial=='Master'),'Age'] = 5\n","# data_train.loc[(data_train.Age.isnull())&(data_train.Initial=='Miss'),'Age'] = 22\n","# data_train.loc[(data_train.Age.isnull())&(data_train.Initial=='Other'),'Age'] = 46\n","\n","# data_test.loc[(data_test.Age.isnull())&(data_test.Initial=='Mr'),'Age'] = 33\n","# data_test.loc[(data_test.Age.isnull())&(data_test.Initial=='Mrs'),'Age'] = 36\n","# data_test.loc[(data_test.Age.isnull())&(data_test.Initial=='Master'),'Age'] = 5\n","# data_test.loc[(data_test.Age.isnull())&(data_test.Initial=='Miss'),'Age'] = 22\n","# data_test.loc[(data_test.Age.isnull())&(data_test.Initial=='Other'),'Age'] = 46\n","def category_age(x):\n","    if x <15:\n","        return 0\n","    elif x < 25:\n","        return 1\n","    elif x < 35:\n","        return 2\n","    elif x < 45:\n","        return 3\n","    elif x < 55:\n","        return 4\n","    elif x < 65:\n","        return 5\n","    elif x < 75:\n","        return 6\n","    else:\n","        return 7    \n","data_train['Age_cate'] = data_train['Age'].apply(category_age)\n","data_test['Age_cate'] = data_test['Age'].apply(category_age)\n","\n","# 성별\n","data_train['Initial'] = data_train['Initial'].replace({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})\n","data_test['Initial'] = data_test['Initial'].replace({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})\n","\n","# Embarked 채워 넣기\n","data_train['Embarked'].fillna('S', inplace=True)\n","\n","# Fare 채워 넣기\n","data_test.loc[data_test.Fare.isnull(), 'Fare'] = data_test['Fare'].median()\n","def category_fare(x):\n","  if x<=30:\n","    return 0\n","  elif x<=80:\n","    return 1\n","  elif x<=100:\n","    return 2\n","  else:\n","    return 3\n","data_train['Fare'] = data_train['Fare'].apply(category_fare)\n","data_test['Fare'] = data_test['Fare'].apply(category_fare)\n","\n","# 문자형식으로 저장되어 있는 성(sex)과 탑승역(embarked)에 대해 숫자로 치환\n","data_train=data_train.replace({\"male\":0,\"female\":1,\"Q\":0,\"S\":1,\"C\":2}) \n","data_test=data_test.replace({\"male\":0,\"female\":1,\"Q\":0,\"S\":1,\"C\":2})\n","\n","print(data_train.head())\n","#print(data_test.isnull().any()) \n"],"execution_count":256,"outputs":[{"output_type":"stream","text":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name  Sex   Age  SibSp  Parch  \\\n","0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n","2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n","4                           Allen, Mr. William Henry    0  35.0      0      0   \n","\n","             Ticket  Fare Cabin  Embarked  Initial  Age_cate  \n","0         A/5 21171     0   NaN         1        2         1  \n","1          PC 17599     1   C85         2        3         3  \n","2  STON/O2. 3101282     0   NaN         1        1         2  \n","3            113803     1  C123         1        3         3  \n","4            373450     0   NaN         1        2         3  \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"],"name":"stderr"}]},{"metadata":{"id":"8MI6hiqrjpYY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":474},"outputId":"a58d6f66-f490-43bf-8609-f016527fe973","executionInfo":{"status":"ok","timestamp":1549519658042,"user_tz":-540,"elapsed":724,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}}},"cell_type":"code","source":["# 정규화 작업\n","from sklearn.preprocessing import StandardScaler\n","data_train['Pclass'] = StandardScaler().fit_transform(data_train['Pclass'].values.reshape(-1, 1))\n","data_train['Sex'] = StandardScaler().fit_transform(data_train['Sex'].values.reshape(-1, 1))\n","data_train['Age'] = StandardScaler().fit_transform(data_train['Age'].values.reshape(-1, 1))\n","data_train['Fare'] = StandardScaler().fit_transform(data_train['Fare'].values.reshape(-1, 1))\n","data_train['Embarked'] = StandardScaler().fit_transform(data_train['Embarked'].values.reshape(-1, 1))\n","data_train['Initial'] = StandardScaler().fit_transform(data_train['Initial'].values.reshape(-1, 1))\n","data_train['Age_cate'] = StandardScaler().fit_transform(data_train['Age_cate'].values.reshape(-1, 1))\n","\n","data_test['Pclass'] = StandardScaler().fit_transform(data_test['Pclass'].values.reshape(-1, 1))\n","data_test['Sex'] = StandardScaler().fit_transform(data_test['Sex'].values.reshape(-1, 1))\n","data_test['Age'] = StandardScaler().fit_transform(data_test['Age'].values.reshape(-1, 1))\n","data_test['Fare'] = StandardScaler().fit_transform(data_test['Fare'].values.reshape(-1, 1))\n","data_test['Embarked'] = StandardScaler().fit_transform(data_test['Embarked'].values.reshape(-1, 1))\n","data_test['Initial'] = StandardScaler().fit_transform(data_test['Initial'].values.reshape(-1, 1))\n","data_test['Age_cate'] = StandardScaler().fit_transform(data_test['Age_cate'].values.reshape(-1, 1))"],"execution_count":262,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n","  warnings.warn(msg, DataConversionWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"yvDy55i1BwNt","colab_type":"code","outputId":"5659b9f3-be56-49e5-b625-e6d1e1e0ee4d","executionInfo":{"status":"ok","timestamp":1549521010885,"user_tz":-540,"elapsed":686,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# 트레이닝할 데이터의 입력 및 출력 데이터 나눔\n","X_train=data_train[[\"Pclass\", \"Initial\", \"Age_cate\", \"Fare\",\"Embarked\"]].as_matrix()\n","Y_train=data_train[[\"Survived\"]].as_matrix()\n","# 입력, 히든, 출력층의 길이 지정\n","len_in=len(X_train[0])\n","len_out= 1#len(Y_train)\n","n_hidden = 100\n","\n","print('in', len_in)\n","# 테스트할 데이터\n","p_id=data_test[[\"PassengerId\"]].as_matrix()\n","X_test = data_test[[\"Pclass\", \"Initial\", \"Age_cate\",\"Fare\",\"Embarked\"]].as_matrix()\n","\n","from sklearn.model_selection import train_test_split\n","rf_X_train = X_train\n","rf_Y_train = Y_train\n","X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=4242)"],"execution_count":272,"outputs":[{"output_type":"stream","text":["in 5\n"],"name":"stdout"}]},{"metadata":{"id":"au8CQJQMvcnd","colab_type":"code","colab":{}},"cell_type":"code","source":["# 모델 생성\n","import keras\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Activation ,Dropout, merge, add\n","from keras.optimizers import SGD, Adam, Adagrad, Adadelta\n","from keras.layers.normalization import BatchNormalization\n","\n","def build_model():\n","  \n","  model = Sequential()\n","  model .add(Dense(n_hidden, input_dim=len_in))\n","  model.add(BatchNormalization())\n","  model .add(Activation('relu'))\n","  #model.add(Dropout(0.3))\n","  \n","  model .add(Dense(n_hidden))\n","  model.add(BatchNormalization())\n","  model .add(Activation('relu'))\n","  #model.add(Dropout(0.3))\n","\n","  model .add(Dense(n_hidden))\n","  model.add(BatchNormalization())\n","  model .add(Activation('relu'))\n","  model.add(Dropout(0.3))\n","\n","  model .add(Dense(1))\n","  model .add(Activation('sigmoid'))\n","\n","  #optimizer = SGD(lr=0.0001, momentum = 0.9, nesterov=True)\n","  #optimizer = Adagrad(lr=0.0001)\n","  #optimizer = Adadelta(rho=0.95)\n","  optimizer = Adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999)\n","  model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","  \n","  return model\n","\n","def build_model2():\n","  rmodel = Sequential()\n","  rmodel.add(Dense(64, input_dim=len_in))\n","  rmodel.add(BatchNormalization())\n","  rmodel.add(Activation('relu'))  \n","  \n","  rmodel.add(Dense(32))\n","  rmodel.add(BatchNormalization())\n","  rmodel.add(Activation('relu'))  \n","  \n","  rmodel.add(Dense(16))\n","  rmodel.add(BatchNormalization())\n","  rmodel.add(Activation('relu'))  \n","  \n","  rmodel.add(Dense(8))\n","  rmodel.add(BatchNormalization())\n","  rmodel.add(Activation('relu'))  \n","  \n","  rmodel.add(Dense(4))\n","  rmodel.add(BatchNormalization())\n","  rmodel.add(Activation('relu')) \n","  \n","  rmodel.add(Dense(1))\n","  rmodel.add(Activation('sigmoid'))\n","  \n","  \n","  lmodel = Sequential()\n","  lmodel.add(Dense(64, input_dim=len_in))\n","  lmodel.add(BatchNormalization())\n","  lmodel.add(Activation('relu'))  \n","  \n","  lmodel.add(Dense(32))\n","  lmodel.add(BatchNormalization())\n","  lmodel.add(Activation('relu'))  \n","  \n","  lmodel.add(Dense(16))\n","  lmodel.add(BatchNormalization())\n","  lmodel.add(Activation('relu'))  \n","  \n","  lmodel.add(Dense(8))\n","  lmodel.add(BatchNormalization())\n","  lmodel.add(Activation('relu'))  \n","  \n","  lmodel.add(Dense(4))\n","  lmodel.add(BatchNormalization())\n","  lmodel.add(Activation('relu')) \n","  \n","  lmodel.add(Dense(1))\n","  lmodel.add(Activation('sigmoid'))\n","  \n","  merge = Sequential()\n","  \n","  merge = Add([lmodel, rmodel])\n","  dense1  = Dense(16, activation='relu')(merge)\n","  dense2  = Dense(8,  activation='relu')(dense1)\n","  dense2a = BatchNormalization()(dense2)\n","  dense3  = Dense(8,  activation='relu')(dense2a)\n","  \n","  output  = Dense(1, activation='sigmoid')(dense3)\n","  \n","  model = Model(inputs=[lmodel, rmodel], outputs=out)\n","  \n","  optimizer = Adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999)\n","  model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","  \n","  \n","  return model\n","# Make models as ensemble\n","from sklearn.ensemble import RandomForestClassifier\n","random_forest = RandomForestClassifier(n_estimators=100)\n","\n","ensemble_size = 1\n","models = []\n","for i in range(ensemble_size):\n","    models.append(build_model())\n","    \n","    \n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"brDcV7wwOQZ0","colab_type":"code","outputId":"82b62648-0c7b-472b-a06a-b48b23e8efb2","executionInfo":{"status":"ok","timestamp":1549519800514,"user_tz":-540,"elapsed":135389,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}},"colab":{"base_uri":"https://localhost:8080/","height":21074}},"cell_type":"code","source":["# training 시작!!\n","from keras.callbacks import EarlyStopping\n","early_stopping = EarlyStopping(monitor = 'val_loss', patience=100, verbose =1) \n","#patience: 오차를 보기 위해서 과거 몇 에폭까지 거슬러 올라가는가\n","\n","epochs = 1000\n","batch_size = 100\n","histories = []  ## for loss curve\n","print('Training 시작!!')\n","for i in range(ensemble_size):\n","    print ('model = ', i)\n","    hist = models[i].fit(X_train, Y_train, validation_data = (X_valid, Y_valid),\n","                         epochs=epochs, batch_size=batch_size, callbacks = [early_stopping])\n","    histories.append(hist)\n","#model.fit(X_train2, Y_train2, epochs=epochs, batch_size=batch_size)\n","print('Training 끝!!')"],"execution_count":265,"outputs":[{"output_type":"stream","text":["Training 시작!!\n","model =  0\n","Train on 712 samples, validate on 179 samples\n","Epoch 1/1000\n","712/712 [==============================] - 48s 67ms/step - loss: 0.8707 - acc: 0.5169 - val_loss: 0.7460 - val_acc: 0.5810\n","Epoch 2/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.8163 - acc: 0.5506 - val_loss: 0.7185 - val_acc: 0.6145\n","Epoch 3/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.7572 - acc: 0.5969 - val_loss: 0.6855 - val_acc: 0.6760\n","Epoch 4/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.7336 - acc: 0.6376 - val_loss: 0.6710 - val_acc: 0.6983\n","Epoch 5/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.7165 - acc: 0.6320 - val_loss: 0.6501 - val_acc: 0.7039\n","Epoch 6/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.7050 - acc: 0.6531 - val_loss: 0.6343 - val_acc: 0.6927\n","Epoch 7/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.6698 - acc: 0.6756 - val_loss: 0.6212 - val_acc: 0.7151\n","Epoch 8/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.6387 - acc: 0.6952 - val_loss: 0.6110 - val_acc: 0.7095\n","Epoch 9/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.6333 - acc: 0.7065 - val_loss: 0.5993 - val_acc: 0.7318\n","Epoch 10/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.6280 - acc: 0.7079 - val_loss: 0.5909 - val_acc: 0.7374\n","Epoch 11/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.5932 - acc: 0.7303 - val_loss: 0.5858 - val_acc: 0.7430\n","Epoch 12/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.6106 - acc: 0.7163 - val_loss: 0.5778 - val_acc: 0.7542\n","Epoch 13/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.5933 - acc: 0.7444 - val_loss: 0.5717 - val_acc: 0.7542\n","Epoch 14/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.5997 - acc: 0.7331 - val_loss: 0.5664 - val_acc: 0.7542\n","Epoch 15/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.5806 - acc: 0.7458 - val_loss: 0.5599 - val_acc: 0.7877\n","Epoch 16/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.5408 - acc: 0.7598 - val_loss: 0.5541 - val_acc: 0.7877\n","Epoch 17/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.5621 - acc: 0.7317 - val_loss: 0.5499 - val_acc: 0.7933\n","Epoch 18/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.5645 - acc: 0.7430 - val_loss: 0.5476 - val_acc: 0.7989\n","Epoch 19/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.5314 - acc: 0.7598 - val_loss: 0.5454 - val_acc: 0.7933\n","Epoch 20/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.5431 - acc: 0.7683 - val_loss: 0.5428 - val_acc: 0.7989\n","Epoch 21/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.5136 - acc: 0.7837 - val_loss: 0.5398 - val_acc: 0.7989\n","Epoch 22/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.5268 - acc: 0.7598 - val_loss: 0.5357 - val_acc: 0.7989\n","Epoch 23/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.5160 - acc: 0.7795 - val_loss: 0.5316 - val_acc: 0.7989\n","Epoch 24/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.4990 - acc: 0.7921 - val_loss: 0.5293 - val_acc: 0.7989\n","Epoch 25/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.5130 - acc: 0.7767 - val_loss: 0.5263 - val_acc: 0.7933\n","Epoch 26/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.5034 - acc: 0.7865 - val_loss: 0.5232 - val_acc: 0.7989\n","Epoch 27/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.5103 - acc: 0.7767 - val_loss: 0.5232 - val_acc: 0.7765\n","Epoch 28/1000\n","712/712 [==============================] - 0s 230us/step - loss: 0.5005 - acc: 0.7809 - val_loss: 0.5257 - val_acc: 0.7821\n","Epoch 29/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.5143 - acc: 0.7767 - val_loss: 0.5253 - val_acc: 0.7821\n","Epoch 30/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.4771 - acc: 0.8034 - val_loss: 0.5242 - val_acc: 0.7821\n","Epoch 31/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.4820 - acc: 0.7795 - val_loss: 0.5209 - val_acc: 0.7765\n","Epoch 32/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.4747 - acc: 0.7893 - val_loss: 0.5154 - val_acc: 0.7821\n","Epoch 33/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.4802 - acc: 0.7837 - val_loss: 0.5120 - val_acc: 0.7709\n","Epoch 34/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4895 - acc: 0.7865 - val_loss: 0.5112 - val_acc: 0.7709\n","Epoch 35/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.4540 - acc: 0.8132 - val_loss: 0.5097 - val_acc: 0.7709\n","Epoch 36/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.4705 - acc: 0.8006 - val_loss: 0.5092 - val_acc: 0.7709\n","Epoch 37/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4419 - acc: 0.8202 - val_loss: 0.5078 - val_acc: 0.7709\n","Epoch 38/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4672 - acc: 0.8146 - val_loss: 0.5033 - val_acc: 0.7654\n","Epoch 39/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.4638 - acc: 0.7992 - val_loss: 0.4997 - val_acc: 0.7654\n","Epoch 40/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.4479 - acc: 0.8020 - val_loss: 0.4990 - val_acc: 0.7654\n","Epoch 41/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.4638 - acc: 0.7879 - val_loss: 0.5001 - val_acc: 0.7709\n","Epoch 42/1000\n","712/712 [==============================] - 0s 224us/step - loss: 0.4392 - acc: 0.8160 - val_loss: 0.5029 - val_acc: 0.7598\n","Epoch 43/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4643 - acc: 0.7809 - val_loss: 0.5014 - val_acc: 0.7598\n","Epoch 44/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.4574 - acc: 0.8006 - val_loss: 0.4991 - val_acc: 0.7598\n","Epoch 45/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.4477 - acc: 0.7963 - val_loss: 0.4970 - val_acc: 0.7765\n","Epoch 46/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.4463 - acc: 0.8062 - val_loss: 0.4950 - val_acc: 0.7765\n","Epoch 47/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.4479 - acc: 0.7992 - val_loss: 0.4968 - val_acc: 0.7765\n","Epoch 48/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.4446 - acc: 0.8020 - val_loss: 0.4968 - val_acc: 0.7654\n","Epoch 49/1000\n","712/712 [==============================] - 0s 226us/step - loss: 0.4402 - acc: 0.7921 - val_loss: 0.4964 - val_acc: 0.7654\n","Epoch 50/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4535 - acc: 0.7949 - val_loss: 0.4985 - val_acc: 0.7654\n","Epoch 51/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4378 - acc: 0.8287 - val_loss: 0.4969 - val_acc: 0.7654\n","Epoch 52/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.4452 - acc: 0.7963 - val_loss: 0.4951 - val_acc: 0.7654\n","Epoch 53/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.4663 - acc: 0.8034 - val_loss: 0.4898 - val_acc: 0.7709\n","Epoch 54/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.4393 - acc: 0.8160 - val_loss: 0.4877 - val_acc: 0.7709\n","Epoch 55/1000\n","712/712 [==============================] - 0s 222us/step - loss: 0.4488 - acc: 0.8104 - val_loss: 0.4893 - val_acc: 0.7654\n","Epoch 56/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.4434 - acc: 0.8118 - val_loss: 0.4905 - val_acc: 0.7654\n","Epoch 57/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.4419 - acc: 0.7949 - val_loss: 0.4886 - val_acc: 0.7654\n","Epoch 58/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.4419 - acc: 0.8146 - val_loss: 0.4871 - val_acc: 0.7598\n","Epoch 59/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.4353 - acc: 0.8034 - val_loss: 0.4854 - val_acc: 0.7598\n","Epoch 60/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.4344 - acc: 0.8048 - val_loss: 0.4850 - val_acc: 0.7654\n","Epoch 61/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.4430 - acc: 0.8146 - val_loss: 0.4826 - val_acc: 0.7765\n","Epoch 62/1000\n","712/712 [==============================] - 0s 230us/step - loss: 0.4328 - acc: 0.7963 - val_loss: 0.4828 - val_acc: 0.7654\n","Epoch 63/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.4249 - acc: 0.7963 - val_loss: 0.4836 - val_acc: 0.7598\n","Epoch 64/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4228 - acc: 0.8174 - val_loss: 0.4829 - val_acc: 0.7598\n","Epoch 65/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.4287 - acc: 0.8160 - val_loss: 0.4798 - val_acc: 0.7598\n","Epoch 66/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.4254 - acc: 0.8216 - val_loss: 0.4786 - val_acc: 0.7598\n","Epoch 67/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.4330 - acc: 0.8132 - val_loss: 0.4764 - val_acc: 0.7709\n","Epoch 68/1000\n","712/712 [==============================] - 0s 224us/step - loss: 0.4178 - acc: 0.8188 - val_loss: 0.4738 - val_acc: 0.7709\n","Epoch 69/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.4367 - acc: 0.8006 - val_loss: 0.4735 - val_acc: 0.7765\n","Epoch 70/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.4247 - acc: 0.8160 - val_loss: 0.4747 - val_acc: 0.7765\n","Epoch 71/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.4314 - acc: 0.8146 - val_loss: 0.4759 - val_acc: 0.7654\n","Epoch 72/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.4258 - acc: 0.8287 - val_loss: 0.4781 - val_acc: 0.7654\n","Epoch 73/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.4214 - acc: 0.8118 - val_loss: 0.4805 - val_acc: 0.7598\n","Epoch 74/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4256 - acc: 0.8216 - val_loss: 0.4821 - val_acc: 0.7598\n","Epoch 75/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.4150 - acc: 0.8258 - val_loss: 0.4819 - val_acc: 0.7709\n","Epoch 76/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.4405 - acc: 0.7865 - val_loss: 0.4799 - val_acc: 0.7709\n","Epoch 77/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.4311 - acc: 0.8090 - val_loss: 0.4808 - val_acc: 0.7709\n","Epoch 78/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.4106 - acc: 0.8244 - val_loss: 0.4781 - val_acc: 0.7709\n","Epoch 79/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.4299 - acc: 0.8048 - val_loss: 0.4749 - val_acc: 0.7709\n","Epoch 80/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.4089 - acc: 0.8343 - val_loss: 0.4752 - val_acc: 0.7709\n","Epoch 81/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.4382 - acc: 0.8104 - val_loss: 0.4736 - val_acc: 0.7765\n","Epoch 82/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.4144 - acc: 0.8188 - val_loss: 0.4718 - val_acc: 0.7765\n","Epoch 83/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4158 - acc: 0.8244 - val_loss: 0.4678 - val_acc: 0.7765\n","Epoch 84/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.4217 - acc: 0.8118 - val_loss: 0.4668 - val_acc: 0.7765\n","Epoch 85/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4024 - acc: 0.8272 - val_loss: 0.4674 - val_acc: 0.7765\n","Epoch 86/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.4165 - acc: 0.8301 - val_loss: 0.4690 - val_acc: 0.7989\n","Epoch 87/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4108 - acc: 0.8188 - val_loss: 0.4695 - val_acc: 0.7989\n","Epoch 88/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.4168 - acc: 0.8230 - val_loss: 0.4676 - val_acc: 0.7989\n","Epoch 89/1000\n","712/712 [==============================] - 0s 232us/step - loss: 0.4135 - acc: 0.8272 - val_loss: 0.4674 - val_acc: 0.7989\n","Epoch 90/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3978 - acc: 0.8287 - val_loss: 0.4673 - val_acc: 0.7989\n","Epoch 91/1000\n","712/712 [==============================] - 0s 238us/step - loss: 0.4188 - acc: 0.8174 - val_loss: 0.4678 - val_acc: 0.7989\n","Epoch 92/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3955 - acc: 0.8160 - val_loss: 0.4695 - val_acc: 0.7821\n","Epoch 93/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.4004 - acc: 0.8202 - val_loss: 0.4717 - val_acc: 0.7821\n","Epoch 94/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4252 - acc: 0.8287 - val_loss: 0.4750 - val_acc: 0.7877\n","Epoch 95/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.4266 - acc: 0.8006 - val_loss: 0.4766 - val_acc: 0.7877\n","Epoch 96/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.4116 - acc: 0.8244 - val_loss: 0.4767 - val_acc: 0.7933\n","Epoch 97/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.4323 - acc: 0.8062 - val_loss: 0.4776 - val_acc: 0.7877\n","Epoch 98/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.4128 - acc: 0.8174 - val_loss: 0.4789 - val_acc: 0.7877\n","Epoch 99/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.4115 - acc: 0.8272 - val_loss: 0.4753 - val_acc: 0.7877\n","Epoch 100/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.4136 - acc: 0.8146 - val_loss: 0.4717 - val_acc: 0.7877\n","Epoch 101/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.3993 - acc: 0.8287 - val_loss: 0.4703 - val_acc: 0.7877\n","Epoch 102/1000\n","712/712 [==============================] - 0s 229us/step - loss: 0.4029 - acc: 0.8371 - val_loss: 0.4693 - val_acc: 0.8045\n","Epoch 103/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.4076 - acc: 0.8174 - val_loss: 0.4670 - val_acc: 0.8045\n","Epoch 104/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.4087 - acc: 0.8230 - val_loss: 0.4662 - val_acc: 0.8045\n","Epoch 105/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.4215 - acc: 0.8076 - val_loss: 0.4651 - val_acc: 0.8045\n","Epoch 106/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.4194 - acc: 0.8146 - val_loss: 0.4655 - val_acc: 0.8045\n","Epoch 107/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.4071 - acc: 0.8230 - val_loss: 0.4663 - val_acc: 0.8045\n","Epoch 108/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3989 - acc: 0.8244 - val_loss: 0.4671 - val_acc: 0.8045\n","Epoch 109/1000\n","712/712 [==============================] - 0s 238us/step - loss: 0.4160 - acc: 0.8090 - val_loss: 0.4692 - val_acc: 0.8045\n","Epoch 110/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.3877 - acc: 0.8202 - val_loss: 0.4715 - val_acc: 0.7933\n","Epoch 111/1000\n","712/712 [==============================] - 0s 224us/step - loss: 0.4080 - acc: 0.8301 - val_loss: 0.4731 - val_acc: 0.7933\n","Epoch 112/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.4077 - acc: 0.8399 - val_loss: 0.4745 - val_acc: 0.7933\n","Epoch 113/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.4159 - acc: 0.8146 - val_loss: 0.4751 - val_acc: 0.7933\n","Epoch 114/1000\n","712/712 [==============================] - 0s 225us/step - loss: 0.3804 - acc: 0.8287 - val_loss: 0.4728 - val_acc: 0.7933\n","Epoch 115/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.4149 - acc: 0.8132 - val_loss: 0.4708 - val_acc: 0.7933\n","Epoch 116/1000\n","712/712 [==============================] - 0s 223us/step - loss: 0.3978 - acc: 0.8329 - val_loss: 0.4709 - val_acc: 0.7933\n","Epoch 117/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.3834 - acc: 0.8329 - val_loss: 0.4696 - val_acc: 0.7933\n","Epoch 118/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.4033 - acc: 0.8202 - val_loss: 0.4696 - val_acc: 0.7933\n","Epoch 119/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3974 - acc: 0.8174 - val_loss: 0.4684 - val_acc: 0.7933\n","Epoch 120/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3952 - acc: 0.8441 - val_loss: 0.4645 - val_acc: 0.8045\n","Epoch 121/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3986 - acc: 0.8287 - val_loss: 0.4611 - val_acc: 0.8045\n","Epoch 122/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.3908 - acc: 0.8272 - val_loss: 0.4593 - val_acc: 0.8045\n","Epoch 123/1000\n","712/712 [==============================] - 0s 223us/step - loss: 0.3890 - acc: 0.8357 - val_loss: 0.4569 - val_acc: 0.8045\n","Epoch 124/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4200 - acc: 0.8202 - val_loss: 0.4573 - val_acc: 0.8045\n","Epoch 125/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3842 - acc: 0.8301 - val_loss: 0.4582 - val_acc: 0.8045\n","Epoch 126/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.3858 - acc: 0.8258 - val_loss: 0.4566 - val_acc: 0.8045\n","Epoch 127/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.4142 - acc: 0.8132 - val_loss: 0.4592 - val_acc: 0.8045\n","Epoch 128/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3919 - acc: 0.8413 - val_loss: 0.4606 - val_acc: 0.8045\n","Epoch 129/1000\n","712/712 [==============================] - 0s 228us/step - loss: 0.3944 - acc: 0.8174 - val_loss: 0.4601 - val_acc: 0.8045\n","Epoch 130/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.4012 - acc: 0.8244 - val_loss: 0.4604 - val_acc: 0.8045\n","Epoch 131/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.4009 - acc: 0.8202 - val_loss: 0.4614 - val_acc: 0.7933\n","Epoch 132/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3888 - acc: 0.8329 - val_loss: 0.4643 - val_acc: 0.7933\n","Epoch 133/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4219 - acc: 0.8076 - val_loss: 0.4640 - val_acc: 0.7933\n","Epoch 134/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.4167 - acc: 0.8174 - val_loss: 0.4622 - val_acc: 0.7933\n","Epoch 135/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.4123 - acc: 0.8132 - val_loss: 0.4591 - val_acc: 0.7989\n","Epoch 136/1000\n","712/712 [==============================] - 0s 224us/step - loss: 0.3935 - acc: 0.8230 - val_loss: 0.4584 - val_acc: 0.7933\n","Epoch 137/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.4127 - acc: 0.8174 - val_loss: 0.4597 - val_acc: 0.7933\n","Epoch 138/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3799 - acc: 0.8371 - val_loss: 0.4607 - val_acc: 0.7933\n","Epoch 139/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3888 - acc: 0.8385 - val_loss: 0.4601 - val_acc: 0.7933\n","Epoch 140/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3917 - acc: 0.8343 - val_loss: 0.4615 - val_acc: 0.7933\n","Epoch 141/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3981 - acc: 0.8202 - val_loss: 0.4611 - val_acc: 0.7933\n","Epoch 142/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3895 - acc: 0.8287 - val_loss: 0.4592 - val_acc: 0.8045\n","Epoch 143/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3995 - acc: 0.8343 - val_loss: 0.4580 - val_acc: 0.8045\n","Epoch 144/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3853 - acc: 0.8315 - val_loss: 0.4588 - val_acc: 0.8045\n","Epoch 145/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3888 - acc: 0.8287 - val_loss: 0.4603 - val_acc: 0.7933\n","Epoch 146/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3921 - acc: 0.8287 - val_loss: 0.4613 - val_acc: 0.7933\n","Epoch 147/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3945 - acc: 0.8258 - val_loss: 0.4641 - val_acc: 0.7989\n","Epoch 148/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3930 - acc: 0.8413 - val_loss: 0.4630 - val_acc: 0.7989\n","Epoch 149/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3795 - acc: 0.8469 - val_loss: 0.4606 - val_acc: 0.7989\n","Epoch 150/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.3992 - acc: 0.8329 - val_loss: 0.4594 - val_acc: 0.7989\n","Epoch 151/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3888 - acc: 0.8385 - val_loss: 0.4601 - val_acc: 0.7933\n","Epoch 152/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3997 - acc: 0.8272 - val_loss: 0.4615 - val_acc: 0.7933\n","Epoch 153/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3887 - acc: 0.8188 - val_loss: 0.4601 - val_acc: 0.7933\n","Epoch 154/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3844 - acc: 0.8427 - val_loss: 0.4587 - val_acc: 0.7933\n","Epoch 155/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3969 - acc: 0.8230 - val_loss: 0.4574 - val_acc: 0.7989\n","Epoch 156/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3971 - acc: 0.8357 - val_loss: 0.4592 - val_acc: 0.7989\n","Epoch 157/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.3750 - acc: 0.8343 - val_loss: 0.4597 - val_acc: 0.7989\n","Epoch 158/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3958 - acc: 0.8272 - val_loss: 0.4619 - val_acc: 0.7933\n","Epoch 159/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.4009 - acc: 0.8202 - val_loss: 0.4635 - val_acc: 0.7933\n","Epoch 160/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3988 - acc: 0.8202 - val_loss: 0.4630 - val_acc: 0.7933\n","Epoch 161/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3835 - acc: 0.8399 - val_loss: 0.4622 - val_acc: 0.7933\n","Epoch 162/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3879 - acc: 0.8413 - val_loss: 0.4622 - val_acc: 0.7933\n","Epoch 163/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3810 - acc: 0.8343 - val_loss: 0.4634 - val_acc: 0.7933\n","Epoch 164/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.4039 - acc: 0.8202 - val_loss: 0.4666 - val_acc: 0.7933\n","Epoch 165/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3934 - acc: 0.8287 - val_loss: 0.4694 - val_acc: 0.7933\n","Epoch 166/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3758 - acc: 0.8371 - val_loss: 0.4689 - val_acc: 0.7933\n","Epoch 167/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3835 - acc: 0.8511 - val_loss: 0.4674 - val_acc: 0.7933\n","Epoch 168/1000\n","712/712 [==============================] - 0s 195us/step - loss: 0.3910 - acc: 0.8188 - val_loss: 0.4644 - val_acc: 0.7989\n","Epoch 169/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3866 - acc: 0.8230 - val_loss: 0.4660 - val_acc: 0.7989\n","Epoch 170/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.4044 - acc: 0.8118 - val_loss: 0.4672 - val_acc: 0.7989\n","Epoch 171/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3846 - acc: 0.8202 - val_loss: 0.4661 - val_acc: 0.7989\n","Epoch 172/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3968 - acc: 0.8174 - val_loss: 0.4673 - val_acc: 0.7989\n","Epoch 173/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3859 - acc: 0.8329 - val_loss: 0.4649 - val_acc: 0.7989\n","Epoch 174/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.4099 - acc: 0.8202 - val_loss: 0.4626 - val_acc: 0.7989\n","Epoch 175/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.4000 - acc: 0.8216 - val_loss: 0.4599 - val_acc: 0.7989\n","Epoch 176/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3927 - acc: 0.8230 - val_loss: 0.4607 - val_acc: 0.7989\n","Epoch 177/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3772 - acc: 0.8413 - val_loss: 0.4608 - val_acc: 0.7989\n","Epoch 178/1000\n","712/712 [==============================] - 0s 232us/step - loss: 0.3870 - acc: 0.8216 - val_loss: 0.4598 - val_acc: 0.7933\n","Epoch 179/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.4051 - acc: 0.8244 - val_loss: 0.4584 - val_acc: 0.7989\n","Epoch 180/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.3805 - acc: 0.8371 - val_loss: 0.4570 - val_acc: 0.7989\n","Epoch 181/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.4032 - acc: 0.8188 - val_loss: 0.4571 - val_acc: 0.7933\n","Epoch 182/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3768 - acc: 0.8371 - val_loss: 0.4566 - val_acc: 0.7933\n","Epoch 183/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3910 - acc: 0.8329 - val_loss: 0.4547 - val_acc: 0.7933\n","Epoch 184/1000\n","712/712 [==============================] - 0s 222us/step - loss: 0.3747 - acc: 0.8413 - val_loss: 0.4536 - val_acc: 0.7933\n","Epoch 185/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3833 - acc: 0.8244 - val_loss: 0.4542 - val_acc: 0.7933\n","Epoch 186/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3783 - acc: 0.8244 - val_loss: 0.4557 - val_acc: 0.7933\n","Epoch 187/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3849 - acc: 0.8272 - val_loss: 0.4571 - val_acc: 0.7933\n","Epoch 188/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3845 - acc: 0.8371 - val_loss: 0.4566 - val_acc: 0.8045\n","Epoch 189/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3968 - acc: 0.8287 - val_loss: 0.4573 - val_acc: 0.8045\n","Epoch 190/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3802 - acc: 0.8287 - val_loss: 0.4566 - val_acc: 0.8045\n","Epoch 191/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3818 - acc: 0.8371 - val_loss: 0.4579 - val_acc: 0.8045\n","Epoch 192/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3868 - acc: 0.8287 - val_loss: 0.4583 - val_acc: 0.8045\n","Epoch 193/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3858 - acc: 0.8343 - val_loss: 0.4597 - val_acc: 0.8045\n","Epoch 194/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3800 - acc: 0.8343 - val_loss: 0.4626 - val_acc: 0.7933\n","Epoch 195/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3866 - acc: 0.8230 - val_loss: 0.4635 - val_acc: 0.7933\n","Epoch 196/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3698 - acc: 0.8371 - val_loss: 0.4625 - val_acc: 0.7933\n","Epoch 197/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3839 - acc: 0.8301 - val_loss: 0.4611 - val_acc: 0.7933\n","Epoch 198/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.4069 - acc: 0.8287 - val_loss: 0.4626 - val_acc: 0.7821\n","Epoch 199/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3800 - acc: 0.8301 - val_loss: 0.4645 - val_acc: 0.7821\n","Epoch 200/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3842 - acc: 0.8272 - val_loss: 0.4640 - val_acc: 0.7933\n","Epoch 201/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3736 - acc: 0.8385 - val_loss: 0.4641 - val_acc: 0.7933\n","Epoch 202/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3748 - acc: 0.8525 - val_loss: 0.4620 - val_acc: 0.7933\n","Epoch 203/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3954 - acc: 0.8132 - val_loss: 0.4610 - val_acc: 0.7933\n","Epoch 204/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3698 - acc: 0.8371 - val_loss: 0.4610 - val_acc: 0.7933\n","Epoch 205/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3816 - acc: 0.8455 - val_loss: 0.4619 - val_acc: 0.7933\n","Epoch 206/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3808 - acc: 0.8343 - val_loss: 0.4622 - val_acc: 0.7933\n","Epoch 207/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3839 - acc: 0.8427 - val_loss: 0.4589 - val_acc: 0.7933\n","Epoch 208/1000\n","712/712 [==============================] - 0s 195us/step - loss: 0.3760 - acc: 0.8343 - val_loss: 0.4558 - val_acc: 0.8045\n","Epoch 209/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3796 - acc: 0.8399 - val_loss: 0.4532 - val_acc: 0.8045\n","Epoch 210/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3677 - acc: 0.8357 - val_loss: 0.4527 - val_acc: 0.8101\n","Epoch 211/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3888 - acc: 0.8315 - val_loss: 0.4549 - val_acc: 0.7989\n","Epoch 212/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.3720 - acc: 0.8371 - val_loss: 0.4575 - val_acc: 0.7933\n","Epoch 213/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3699 - acc: 0.8329 - val_loss: 0.4607 - val_acc: 0.7933\n","Epoch 214/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3818 - acc: 0.8511 - val_loss: 0.4599 - val_acc: 0.7933\n","Epoch 215/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3587 - acc: 0.8525 - val_loss: 0.4604 - val_acc: 0.7989\n","Epoch 216/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3689 - acc: 0.8329 - val_loss: 0.4566 - val_acc: 0.7989\n","Epoch 217/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3917 - acc: 0.8202 - val_loss: 0.4552 - val_acc: 0.7989\n","Epoch 218/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3729 - acc: 0.8343 - val_loss: 0.4554 - val_acc: 0.7989\n","Epoch 219/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.3785 - acc: 0.8413 - val_loss: 0.4555 - val_acc: 0.7933\n","Epoch 220/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3967 - acc: 0.8399 - val_loss: 0.4571 - val_acc: 0.7933\n","Epoch 221/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.3832 - acc: 0.8315 - val_loss: 0.4573 - val_acc: 0.7933\n","Epoch 222/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3817 - acc: 0.8287 - val_loss: 0.4556 - val_acc: 0.7989\n","Epoch 223/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3873 - acc: 0.8385 - val_loss: 0.4545 - val_acc: 0.7989\n","Epoch 224/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3793 - acc: 0.8441 - val_loss: 0.4545 - val_acc: 0.7989\n","Epoch 225/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3753 - acc: 0.8357 - val_loss: 0.4556 - val_acc: 0.7989\n","Epoch 226/1000\n","712/712 [==============================] - 0s 224us/step - loss: 0.3637 - acc: 0.8272 - val_loss: 0.4541 - val_acc: 0.7989\n","Epoch 227/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3889 - acc: 0.8202 - val_loss: 0.4538 - val_acc: 0.7989\n","Epoch 228/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3864 - acc: 0.8427 - val_loss: 0.4534 - val_acc: 0.7989\n","Epoch 229/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3858 - acc: 0.8315 - val_loss: 0.4544 - val_acc: 0.7989\n","Epoch 230/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3716 - acc: 0.8469 - val_loss: 0.4518 - val_acc: 0.7989\n","Epoch 231/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3716 - acc: 0.8371 - val_loss: 0.4493 - val_acc: 0.7989\n","Epoch 232/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3685 - acc: 0.8357 - val_loss: 0.4483 - val_acc: 0.8101\n","Epoch 233/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3773 - acc: 0.8315 - val_loss: 0.4489 - val_acc: 0.8101\n","Epoch 234/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3827 - acc: 0.8357 - val_loss: 0.4508 - val_acc: 0.7989\n","Epoch 235/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3702 - acc: 0.8343 - val_loss: 0.4546 - val_acc: 0.7989\n","Epoch 236/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3883 - acc: 0.8258 - val_loss: 0.4534 - val_acc: 0.7989\n","Epoch 237/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3833 - acc: 0.8301 - val_loss: 0.4499 - val_acc: 0.7989\n","Epoch 238/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3784 - acc: 0.8230 - val_loss: 0.4480 - val_acc: 0.7989\n","Epoch 239/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3777 - acc: 0.8497 - val_loss: 0.4468 - val_acc: 0.8101\n","Epoch 240/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3803 - acc: 0.8287 - val_loss: 0.4492 - val_acc: 0.8101\n","Epoch 241/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3825 - acc: 0.8301 - val_loss: 0.4515 - val_acc: 0.7989\n","Epoch 242/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3734 - acc: 0.8287 - val_loss: 0.4561 - val_acc: 0.7989\n","Epoch 243/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3771 - acc: 0.8315 - val_loss: 0.4585 - val_acc: 0.7989\n","Epoch 244/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3794 - acc: 0.8301 - val_loss: 0.4611 - val_acc: 0.7989\n","Epoch 245/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3791 - acc: 0.8301 - val_loss: 0.4653 - val_acc: 0.7989\n","Epoch 246/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3748 - acc: 0.8371 - val_loss: 0.4629 - val_acc: 0.7989\n","Epoch 247/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.3945 - acc: 0.8132 - val_loss: 0.4625 - val_acc: 0.7989\n","Epoch 248/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3555 - acc: 0.8455 - val_loss: 0.4612 - val_acc: 0.7989\n","Epoch 249/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3771 - acc: 0.8272 - val_loss: 0.4582 - val_acc: 0.7989\n","Epoch 250/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3839 - acc: 0.8188 - val_loss: 0.4578 - val_acc: 0.7989\n","Epoch 251/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3886 - acc: 0.8385 - val_loss: 0.4551 - val_acc: 0.7989\n","Epoch 252/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3682 - acc: 0.8357 - val_loss: 0.4539 - val_acc: 0.7989\n","Epoch 253/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3708 - acc: 0.8469 - val_loss: 0.4521 - val_acc: 0.7989\n","Epoch 254/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.3709 - acc: 0.8441 - val_loss: 0.4505 - val_acc: 0.7989\n","Epoch 255/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3756 - acc: 0.8469 - val_loss: 0.4499 - val_acc: 0.7989\n","Epoch 256/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3953 - acc: 0.8202 - val_loss: 0.4503 - val_acc: 0.7989\n","Epoch 257/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3822 - acc: 0.8301 - val_loss: 0.4504 - val_acc: 0.7933\n","Epoch 258/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.3677 - acc: 0.8455 - val_loss: 0.4509 - val_acc: 0.7933\n","Epoch 259/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3834 - acc: 0.8216 - val_loss: 0.4513 - val_acc: 0.7989\n","Epoch 260/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3691 - acc: 0.8399 - val_loss: 0.4531 - val_acc: 0.7989\n","Epoch 261/1000\n","712/712 [==============================] - 0s 228us/step - loss: 0.3942 - acc: 0.8202 - val_loss: 0.4524 - val_acc: 0.7989\n","Epoch 262/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3643 - acc: 0.8483 - val_loss: 0.4493 - val_acc: 0.7989\n","Epoch 263/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3714 - acc: 0.8413 - val_loss: 0.4479 - val_acc: 0.8101\n","Epoch 264/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3602 - acc: 0.8427 - val_loss: 0.4487 - val_acc: 0.7989\n","Epoch 265/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3811 - acc: 0.8483 - val_loss: 0.4489 - val_acc: 0.7933\n","Epoch 266/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3698 - acc: 0.8343 - val_loss: 0.4498 - val_acc: 0.7933\n","Epoch 267/1000\n","712/712 [==============================] - 0s 226us/step - loss: 0.3706 - acc: 0.8385 - val_loss: 0.4513 - val_acc: 0.7933\n","Epoch 268/1000\n","712/712 [==============================] - 0s 228us/step - loss: 0.3712 - acc: 0.8343 - val_loss: 0.4512 - val_acc: 0.7933\n","Epoch 269/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.3731 - acc: 0.8455 - val_loss: 0.4514 - val_acc: 0.7933\n","Epoch 270/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3805 - acc: 0.8511 - val_loss: 0.4511 - val_acc: 0.7933\n","Epoch 271/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3546 - acc: 0.8497 - val_loss: 0.4511 - val_acc: 0.7989\n","Epoch 272/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3732 - acc: 0.8329 - val_loss: 0.4510 - val_acc: 0.7989\n","Epoch 273/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3737 - acc: 0.8413 - val_loss: 0.4477 - val_acc: 0.8101\n","Epoch 274/1000\n","712/712 [==============================] - 0s 227us/step - loss: 0.3777 - acc: 0.8455 - val_loss: 0.4460 - val_acc: 0.8101\n","Epoch 275/1000\n","712/712 [==============================] - 0s 194us/step - loss: 0.3938 - acc: 0.8287 - val_loss: 0.4477 - val_acc: 0.7989\n","Epoch 276/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3660 - acc: 0.8413 - val_loss: 0.4472 - val_acc: 0.7989\n","Epoch 277/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3791 - acc: 0.8258 - val_loss: 0.4476 - val_acc: 0.7989\n","Epoch 278/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3746 - acc: 0.8385 - val_loss: 0.4507 - val_acc: 0.7989\n","Epoch 279/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.3666 - acc: 0.8483 - val_loss: 0.4510 - val_acc: 0.7989\n","Epoch 280/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3730 - acc: 0.8455 - val_loss: 0.4517 - val_acc: 0.7989\n","Epoch 281/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3818 - acc: 0.8329 - val_loss: 0.4496 - val_acc: 0.7989\n","Epoch 282/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3674 - acc: 0.8441 - val_loss: 0.4497 - val_acc: 0.7989\n","Epoch 283/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3759 - acc: 0.8455 - val_loss: 0.4501 - val_acc: 0.7989\n","Epoch 284/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3608 - acc: 0.8441 - val_loss: 0.4509 - val_acc: 0.8101\n","Epoch 285/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3662 - acc: 0.8525 - val_loss: 0.4498 - val_acc: 0.8045\n","Epoch 286/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3732 - acc: 0.8399 - val_loss: 0.4510 - val_acc: 0.8101\n","Epoch 287/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3788 - acc: 0.8357 - val_loss: 0.4506 - val_acc: 0.7989\n","Epoch 288/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.3691 - acc: 0.8413 - val_loss: 0.4510 - val_acc: 0.7989\n","Epoch 289/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3617 - acc: 0.8315 - val_loss: 0.4505 - val_acc: 0.7989\n","Epoch 290/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3803 - acc: 0.8301 - val_loss: 0.4500 - val_acc: 0.8101\n","Epoch 291/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3691 - acc: 0.8301 - val_loss: 0.4478 - val_acc: 0.8101\n","Epoch 292/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3654 - acc: 0.8371 - val_loss: 0.4473 - val_acc: 0.8101\n","Epoch 293/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3689 - acc: 0.8455 - val_loss: 0.4460 - val_acc: 0.8101\n","Epoch 294/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3604 - acc: 0.8525 - val_loss: 0.4451 - val_acc: 0.8101\n","Epoch 295/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3752 - acc: 0.8413 - val_loss: 0.4457 - val_acc: 0.7989\n","Epoch 296/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3706 - acc: 0.8553 - val_loss: 0.4448 - val_acc: 0.8101\n","Epoch 297/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3810 - acc: 0.8315 - val_loss: 0.4461 - val_acc: 0.8101\n","Epoch 298/1000\n","712/712 [==============================] - 0s 195us/step - loss: 0.3832 - acc: 0.8357 - val_loss: 0.4461 - val_acc: 0.7989\n","Epoch 299/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3693 - acc: 0.8315 - val_loss: 0.4451 - val_acc: 0.7989\n","Epoch 300/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3807 - acc: 0.8385 - val_loss: 0.4456 - val_acc: 0.7989\n","Epoch 301/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3676 - acc: 0.8427 - val_loss: 0.4468 - val_acc: 0.8101\n","Epoch 302/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3648 - acc: 0.8511 - val_loss: 0.4487 - val_acc: 0.8101\n","Epoch 303/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3821 - acc: 0.8244 - val_loss: 0.4497 - val_acc: 0.8101\n","Epoch 304/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3760 - acc: 0.8287 - val_loss: 0.4543 - val_acc: 0.8101\n","Epoch 305/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3759 - acc: 0.8385 - val_loss: 0.4551 - val_acc: 0.8101\n","Epoch 306/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3517 - acc: 0.8483 - val_loss: 0.4555 - val_acc: 0.8101\n","Epoch 307/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3757 - acc: 0.8385 - val_loss: 0.4557 - val_acc: 0.8101\n","Epoch 308/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3797 - acc: 0.8343 - val_loss: 0.4542 - val_acc: 0.8045\n","Epoch 309/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.3693 - acc: 0.8301 - val_loss: 0.4531 - val_acc: 0.7989\n","Epoch 310/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3860 - acc: 0.8357 - val_loss: 0.4531 - val_acc: 0.7989\n","Epoch 311/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3656 - acc: 0.8469 - val_loss: 0.4521 - val_acc: 0.7989\n","Epoch 312/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3662 - acc: 0.8315 - val_loss: 0.4498 - val_acc: 0.7989\n","Epoch 313/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3653 - acc: 0.8525 - val_loss: 0.4497 - val_acc: 0.7989\n","Epoch 314/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3637 - acc: 0.8399 - val_loss: 0.4504 - val_acc: 0.7989\n","Epoch 315/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3838 - acc: 0.8329 - val_loss: 0.4509 - val_acc: 0.7989\n","Epoch 316/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.3540 - acc: 0.8399 - val_loss: 0.4500 - val_acc: 0.7989\n","Epoch 317/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3829 - acc: 0.8343 - val_loss: 0.4497 - val_acc: 0.8045\n","Epoch 318/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.3727 - acc: 0.8301 - val_loss: 0.4491 - val_acc: 0.8045\n","Epoch 319/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3759 - acc: 0.8357 - val_loss: 0.4477 - val_acc: 0.8101\n","Epoch 320/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3832 - acc: 0.8258 - val_loss: 0.4492 - val_acc: 0.8101\n","Epoch 321/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3868 - acc: 0.8272 - val_loss: 0.4500 - val_acc: 0.8101\n","Epoch 322/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3722 - acc: 0.8441 - val_loss: 0.4493 - val_acc: 0.7933\n","Epoch 323/1000\n","712/712 [==============================] - 0s 226us/step - loss: 0.3753 - acc: 0.8315 - val_loss: 0.4467 - val_acc: 0.7989\n","Epoch 324/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.3641 - acc: 0.8385 - val_loss: 0.4450 - val_acc: 0.7989\n","Epoch 325/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3717 - acc: 0.8399 - val_loss: 0.4448 - val_acc: 0.7989\n","Epoch 326/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3704 - acc: 0.8399 - val_loss: 0.4449 - val_acc: 0.8101\n","Epoch 327/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3604 - acc: 0.8483 - val_loss: 0.4460 - val_acc: 0.8156\n","Epoch 328/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3607 - acc: 0.8399 - val_loss: 0.4490 - val_acc: 0.8045\n","Epoch 329/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3610 - acc: 0.8596 - val_loss: 0.4489 - val_acc: 0.8101\n","Epoch 330/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.3812 - acc: 0.8371 - val_loss: 0.4487 - val_acc: 0.8101\n","Epoch 331/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3737 - acc: 0.8441 - val_loss: 0.4489 - val_acc: 0.8101\n","Epoch 332/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.3668 - acc: 0.8371 - val_loss: 0.4505 - val_acc: 0.8156\n","Epoch 333/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3628 - acc: 0.8315 - val_loss: 0.4524 - val_acc: 0.8045\n","Epoch 334/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3743 - acc: 0.8399 - val_loss: 0.4527 - val_acc: 0.8045\n","Epoch 335/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3646 - acc: 0.8511 - val_loss: 0.4522 - val_acc: 0.8045\n","Epoch 336/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3725 - acc: 0.8357 - val_loss: 0.4522 - val_acc: 0.8045\n","Epoch 337/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3742 - acc: 0.8427 - val_loss: 0.4524 - val_acc: 0.8045\n","Epoch 338/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3865 - acc: 0.8230 - val_loss: 0.4522 - val_acc: 0.8045\n","Epoch 339/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3913 - acc: 0.8301 - val_loss: 0.4513 - val_acc: 0.8045\n","Epoch 340/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3565 - acc: 0.8455 - val_loss: 0.4513 - val_acc: 0.8045\n","Epoch 341/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3708 - acc: 0.8357 - val_loss: 0.4522 - val_acc: 0.8045\n","Epoch 342/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3685 - acc: 0.8469 - val_loss: 0.4561 - val_acc: 0.7989\n","Epoch 343/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3752 - acc: 0.8455 - val_loss: 0.4553 - val_acc: 0.7989\n","Epoch 344/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3839 - acc: 0.8329 - val_loss: 0.4533 - val_acc: 0.7989\n","Epoch 345/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3704 - acc: 0.8399 - val_loss: 0.4507 - val_acc: 0.7989\n","Epoch 346/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3624 - acc: 0.8511 - val_loss: 0.4497 - val_acc: 0.8045\n","Epoch 347/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3607 - acc: 0.8427 - val_loss: 0.4484 - val_acc: 0.8045\n","Epoch 348/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3672 - acc: 0.8441 - val_loss: 0.4466 - val_acc: 0.8045\n","Epoch 349/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3761 - acc: 0.8385 - val_loss: 0.4457 - val_acc: 0.7989\n","Epoch 350/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3708 - acc: 0.8441 - val_loss: 0.4459 - val_acc: 0.8045\n","Epoch 351/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3571 - acc: 0.8511 - val_loss: 0.4464 - val_acc: 0.8045\n","Epoch 352/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3616 - acc: 0.8455 - val_loss: 0.4485 - val_acc: 0.8045\n","Epoch 353/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3687 - acc: 0.8343 - val_loss: 0.4483 - val_acc: 0.8045\n","Epoch 354/1000\n","712/712 [==============================] - 0s 195us/step - loss: 0.3669 - acc: 0.8357 - val_loss: 0.4475 - val_acc: 0.8045\n","Epoch 355/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3568 - acc: 0.8511 - val_loss: 0.4464 - val_acc: 0.8045\n","Epoch 356/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3830 - acc: 0.8511 - val_loss: 0.4456 - val_acc: 0.8045\n","Epoch 357/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3689 - acc: 0.8553 - val_loss: 0.4463 - val_acc: 0.7989\n","Epoch 358/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.3623 - acc: 0.8343 - val_loss: 0.4455 - val_acc: 0.7989\n","Epoch 359/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3708 - acc: 0.8315 - val_loss: 0.4440 - val_acc: 0.7989\n","Epoch 360/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.3646 - acc: 0.8539 - val_loss: 0.4445 - val_acc: 0.7989\n","Epoch 361/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3742 - acc: 0.8399 - val_loss: 0.4455 - val_acc: 0.8045\n","Epoch 362/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3624 - acc: 0.8469 - val_loss: 0.4475 - val_acc: 0.8045\n","Epoch 363/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3639 - acc: 0.8427 - val_loss: 0.4492 - val_acc: 0.8045\n","Epoch 364/1000\n","712/712 [==============================] - 0s 225us/step - loss: 0.3645 - acc: 0.8469 - val_loss: 0.4505 - val_acc: 0.8045\n","Epoch 365/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3732 - acc: 0.8441 - val_loss: 0.4528 - val_acc: 0.8045\n","Epoch 366/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3757 - acc: 0.8301 - val_loss: 0.4543 - val_acc: 0.8045\n","Epoch 367/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3748 - acc: 0.8413 - val_loss: 0.4557 - val_acc: 0.8101\n","Epoch 368/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3705 - acc: 0.8483 - val_loss: 0.4573 - val_acc: 0.8101\n","Epoch 369/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3729 - acc: 0.8301 - val_loss: 0.4553 - val_acc: 0.8101\n","Epoch 370/1000\n","712/712 [==============================] - 0s 211us/step - loss: 0.3700 - acc: 0.8371 - val_loss: 0.4541 - val_acc: 0.8101\n","Epoch 371/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3498 - acc: 0.8567 - val_loss: 0.4506 - val_acc: 0.8045\n","Epoch 372/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3632 - acc: 0.8497 - val_loss: 0.4457 - val_acc: 0.7989\n","Epoch 373/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3807 - acc: 0.8371 - val_loss: 0.4444 - val_acc: 0.8101\n","Epoch 374/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3715 - acc: 0.8427 - val_loss: 0.4467 - val_acc: 0.8045\n","Epoch 375/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3636 - acc: 0.8441 - val_loss: 0.4495 - val_acc: 0.8045\n","Epoch 376/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3671 - acc: 0.8483 - val_loss: 0.4500 - val_acc: 0.8101\n","Epoch 377/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3721 - acc: 0.8413 - val_loss: 0.4477 - val_acc: 0.8101\n","Epoch 378/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3421 - acc: 0.8567 - val_loss: 0.4478 - val_acc: 0.8101\n","Epoch 379/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3591 - acc: 0.8483 - val_loss: 0.4478 - val_acc: 0.7989\n","Epoch 380/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3618 - acc: 0.8497 - val_loss: 0.4499 - val_acc: 0.7933\n","Epoch 381/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3621 - acc: 0.8469 - val_loss: 0.4510 - val_acc: 0.7989\n","Epoch 382/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3547 - acc: 0.8469 - val_loss: 0.4510 - val_acc: 0.7989\n","Epoch 383/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3677 - acc: 0.8441 - val_loss: 0.4497 - val_acc: 0.7933\n","Epoch 384/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3767 - acc: 0.8469 - val_loss: 0.4497 - val_acc: 0.7989\n","Epoch 385/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3595 - acc: 0.8469 - val_loss: 0.4491 - val_acc: 0.7989\n","Epoch 386/1000\n","712/712 [==============================] - 0s 240us/step - loss: 0.3641 - acc: 0.8357 - val_loss: 0.4479 - val_acc: 0.7989\n","Epoch 387/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3750 - acc: 0.8244 - val_loss: 0.4472 - val_acc: 0.7989\n","Epoch 388/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3723 - acc: 0.8399 - val_loss: 0.4474 - val_acc: 0.7989\n","Epoch 389/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3657 - acc: 0.8371 - val_loss: 0.4487 - val_acc: 0.7989\n","Epoch 390/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3717 - acc: 0.8441 - val_loss: 0.4487 - val_acc: 0.7989\n","Epoch 391/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3542 - acc: 0.8483 - val_loss: 0.4481 - val_acc: 0.8101\n","Epoch 392/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3567 - acc: 0.8427 - val_loss: 0.4483 - val_acc: 0.8045\n","Epoch 393/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3611 - acc: 0.8441 - val_loss: 0.4480 - val_acc: 0.8045\n","Epoch 394/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3742 - acc: 0.8399 - val_loss: 0.4490 - val_acc: 0.8045\n","Epoch 395/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3755 - acc: 0.8343 - val_loss: 0.4481 - val_acc: 0.7989\n","Epoch 396/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3602 - acc: 0.8385 - val_loss: 0.4482 - val_acc: 0.7989\n","Epoch 397/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3609 - acc: 0.8469 - val_loss: 0.4473 - val_acc: 0.8045\n","Epoch 398/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3463 - acc: 0.8483 - val_loss: 0.4464 - val_acc: 0.7989\n","Epoch 399/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3722 - acc: 0.8371 - val_loss: 0.4466 - val_acc: 0.7989\n","Epoch 400/1000\n","712/712 [==============================] - 0s 222us/step - loss: 0.3583 - acc: 0.8525 - val_loss: 0.4447 - val_acc: 0.8101\n","Epoch 401/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3787 - acc: 0.8301 - val_loss: 0.4451 - val_acc: 0.7933\n","Epoch 402/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3650 - acc: 0.8483 - val_loss: 0.4463 - val_acc: 0.7933\n","Epoch 403/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3623 - acc: 0.8441 - val_loss: 0.4460 - val_acc: 0.7989\n","Epoch 404/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3636 - acc: 0.8357 - val_loss: 0.4480 - val_acc: 0.8101\n","Epoch 405/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3656 - acc: 0.8483 - val_loss: 0.4482 - val_acc: 0.8101\n","Epoch 406/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3791 - acc: 0.8315 - val_loss: 0.4478 - val_acc: 0.8101\n","Epoch 407/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3699 - acc: 0.8301 - val_loss: 0.4470 - val_acc: 0.8101\n","Epoch 408/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3549 - acc: 0.8483 - val_loss: 0.4457 - val_acc: 0.8212\n","Epoch 409/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3510 - acc: 0.8567 - val_loss: 0.4466 - val_acc: 0.8101\n","Epoch 410/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3666 - acc: 0.8371 - val_loss: 0.4467 - val_acc: 0.8101\n","Epoch 411/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3542 - acc: 0.8497 - val_loss: 0.4464 - val_acc: 0.8101\n","Epoch 412/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3607 - acc: 0.8455 - val_loss: 0.4441 - val_acc: 0.8156\n","Epoch 413/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.3536 - acc: 0.8371 - val_loss: 0.4447 - val_acc: 0.8156\n","Epoch 414/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3548 - acc: 0.8469 - val_loss: 0.4474 - val_acc: 0.8156\n","Epoch 415/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3577 - acc: 0.8441 - val_loss: 0.4473 - val_acc: 0.7989\n","Epoch 416/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3616 - acc: 0.8427 - val_loss: 0.4487 - val_acc: 0.7989\n","Epoch 417/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3541 - acc: 0.8441 - val_loss: 0.4486 - val_acc: 0.7989\n","Epoch 418/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3704 - acc: 0.8511 - val_loss: 0.4488 - val_acc: 0.7989\n","Epoch 419/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3485 - acc: 0.8525 - val_loss: 0.4501 - val_acc: 0.7989\n","Epoch 420/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3709 - acc: 0.8371 - val_loss: 0.4471 - val_acc: 0.8045\n","Epoch 421/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3468 - acc: 0.8553 - val_loss: 0.4456 - val_acc: 0.8045\n","Epoch 422/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3644 - acc: 0.8511 - val_loss: 0.4458 - val_acc: 0.8101\n","Epoch 423/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3629 - acc: 0.8385 - val_loss: 0.4469 - val_acc: 0.8101\n","Epoch 424/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3715 - acc: 0.8357 - val_loss: 0.4499 - val_acc: 0.8101\n","Epoch 425/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3720 - acc: 0.8441 - val_loss: 0.4489 - val_acc: 0.8101\n","Epoch 426/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3552 - acc: 0.8511 - val_loss: 0.4475 - val_acc: 0.8101\n","Epoch 427/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3557 - acc: 0.8525 - val_loss: 0.4447 - val_acc: 0.8101\n","Epoch 428/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3756 - acc: 0.8385 - val_loss: 0.4434 - val_acc: 0.8101\n","Epoch 429/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3577 - acc: 0.8483 - val_loss: 0.4439 - val_acc: 0.8101\n","Epoch 430/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3648 - acc: 0.8539 - val_loss: 0.4457 - val_acc: 0.8045\n","Epoch 431/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3777 - acc: 0.8413 - val_loss: 0.4473 - val_acc: 0.8045\n","Epoch 432/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3825 - acc: 0.8413 - val_loss: 0.4491 - val_acc: 0.8045\n","Epoch 433/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3724 - acc: 0.8371 - val_loss: 0.4500 - val_acc: 0.8101\n","Epoch 434/1000\n","712/712 [==============================] - 0s 222us/step - loss: 0.3518 - acc: 0.8525 - val_loss: 0.4512 - val_acc: 0.8045\n","Epoch 435/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3568 - acc: 0.8399 - val_loss: 0.4525 - val_acc: 0.8045\n","Epoch 436/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3440 - acc: 0.8624 - val_loss: 0.4522 - val_acc: 0.7989\n","Epoch 437/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3813 - acc: 0.8315 - val_loss: 0.4506 - val_acc: 0.7989\n","Epoch 438/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3640 - acc: 0.8301 - val_loss: 0.4490 - val_acc: 0.7989\n","Epoch 439/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3609 - acc: 0.8483 - val_loss: 0.4493 - val_acc: 0.7989\n","Epoch 440/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3703 - acc: 0.8385 - val_loss: 0.4494 - val_acc: 0.7989\n","Epoch 441/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.3870 - acc: 0.8301 - val_loss: 0.4461 - val_acc: 0.7989\n","Epoch 442/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3609 - acc: 0.8357 - val_loss: 0.4462 - val_acc: 0.7989\n","Epoch 443/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3684 - acc: 0.8469 - val_loss: 0.4452 - val_acc: 0.7933\n","Epoch 444/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.3686 - acc: 0.8455 - val_loss: 0.4443 - val_acc: 0.7933\n","Epoch 445/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3584 - acc: 0.8483 - val_loss: 0.4436 - val_acc: 0.7989\n","Epoch 446/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3623 - acc: 0.8399 - val_loss: 0.4452 - val_acc: 0.7989\n","Epoch 447/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3445 - acc: 0.8539 - val_loss: 0.4468 - val_acc: 0.7989\n","Epoch 448/1000\n","712/712 [==============================] - 0s 213us/step - loss: 0.3603 - acc: 0.8441 - val_loss: 0.4497 - val_acc: 0.7989\n","Epoch 449/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.3615 - acc: 0.8371 - val_loss: 0.4529 - val_acc: 0.7989\n","Epoch 450/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3618 - acc: 0.8427 - val_loss: 0.4523 - val_acc: 0.7989\n","Epoch 451/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3493 - acc: 0.8469 - val_loss: 0.4512 - val_acc: 0.7989\n","Epoch 452/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3631 - acc: 0.8469 - val_loss: 0.4490 - val_acc: 0.7989\n","Epoch 453/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3687 - acc: 0.8427 - val_loss: 0.4485 - val_acc: 0.7989\n","Epoch 454/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3621 - acc: 0.8483 - val_loss: 0.4482 - val_acc: 0.7989\n","Epoch 455/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.3722 - acc: 0.8357 - val_loss: 0.4460 - val_acc: 0.7989\n","Epoch 456/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3676 - acc: 0.8385 - val_loss: 0.4495 - val_acc: 0.7933\n","Epoch 457/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3554 - acc: 0.8455 - val_loss: 0.4496 - val_acc: 0.7933\n","Epoch 458/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3678 - acc: 0.8455 - val_loss: 0.4496 - val_acc: 0.7933\n","Epoch 459/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.3566 - acc: 0.8427 - val_loss: 0.4474 - val_acc: 0.7877\n","Epoch 460/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.3574 - acc: 0.8511 - val_loss: 0.4481 - val_acc: 0.7933\n","Epoch 461/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3669 - acc: 0.8413 - val_loss: 0.4522 - val_acc: 0.7877\n","Epoch 462/1000\n","712/712 [==============================] - 0s 235us/step - loss: 0.3661 - acc: 0.8329 - val_loss: 0.4539 - val_acc: 0.7933\n","Epoch 463/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3539 - acc: 0.8525 - val_loss: 0.4572 - val_acc: 0.7989\n","Epoch 464/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3584 - acc: 0.8427 - val_loss: 0.4565 - val_acc: 0.7989\n","Epoch 465/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3713 - acc: 0.8343 - val_loss: 0.4554 - val_acc: 0.7989\n","Epoch 466/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3624 - acc: 0.8441 - val_loss: 0.4548 - val_acc: 0.7989\n","Epoch 467/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3738 - acc: 0.8525 - val_loss: 0.4549 - val_acc: 0.7933\n","Epoch 468/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3620 - acc: 0.8511 - val_loss: 0.4552 - val_acc: 0.7989\n","Epoch 469/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3742 - acc: 0.8357 - val_loss: 0.4548 - val_acc: 0.7989\n","Epoch 470/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.3698 - acc: 0.8413 - val_loss: 0.4573 - val_acc: 0.7989\n","Epoch 471/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3672 - acc: 0.8455 - val_loss: 0.4596 - val_acc: 0.7989\n","Epoch 472/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3616 - acc: 0.8315 - val_loss: 0.4571 - val_acc: 0.7989\n","Epoch 473/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3685 - acc: 0.8357 - val_loss: 0.4494 - val_acc: 0.7989\n","Epoch 474/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3612 - acc: 0.8399 - val_loss: 0.4450 - val_acc: 0.8101\n","Epoch 475/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3658 - acc: 0.8343 - val_loss: 0.4426 - val_acc: 0.8101\n","Epoch 476/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.3553 - acc: 0.8455 - val_loss: 0.4403 - val_acc: 0.8101\n","Epoch 477/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3602 - acc: 0.8469 - val_loss: 0.4426 - val_acc: 0.8101\n","Epoch 478/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3720 - acc: 0.8427 - val_loss: 0.4438 - val_acc: 0.8101\n","Epoch 479/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3742 - acc: 0.8329 - val_loss: 0.4424 - val_acc: 0.8101\n","Epoch 480/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3557 - acc: 0.8483 - val_loss: 0.4427 - val_acc: 0.8045\n","Epoch 481/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3666 - acc: 0.8441 - val_loss: 0.4443 - val_acc: 0.8045\n","Epoch 482/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3586 - acc: 0.8413 - val_loss: 0.4450 - val_acc: 0.7933\n","Epoch 483/1000\n","712/712 [==============================] - 0s 224us/step - loss: 0.3605 - acc: 0.8497 - val_loss: 0.4456 - val_acc: 0.7933\n","Epoch 484/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3695 - acc: 0.8455 - val_loss: 0.4445 - val_acc: 0.8045\n","Epoch 485/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3633 - acc: 0.8413 - val_loss: 0.4461 - val_acc: 0.7989\n","Epoch 486/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3552 - acc: 0.8525 - val_loss: 0.4486 - val_acc: 0.8101\n","Epoch 487/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3733 - acc: 0.8427 - val_loss: 0.4523 - val_acc: 0.8156\n","Epoch 488/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.3650 - acc: 0.8357 - val_loss: 0.4551 - val_acc: 0.8045\n","Epoch 489/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3632 - acc: 0.8455 - val_loss: 0.4549 - val_acc: 0.7989\n","Epoch 490/1000\n","712/712 [==============================] - 0s 230us/step - loss: 0.3665 - acc: 0.8469 - val_loss: 0.4549 - val_acc: 0.7989\n","Epoch 491/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3670 - acc: 0.8399 - val_loss: 0.4554 - val_acc: 0.7989\n","Epoch 492/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3554 - acc: 0.8385 - val_loss: 0.4541 - val_acc: 0.7989\n","Epoch 493/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3626 - acc: 0.8385 - val_loss: 0.4511 - val_acc: 0.7989\n","Epoch 494/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3478 - acc: 0.8525 - val_loss: 0.4500 - val_acc: 0.7989\n","Epoch 495/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.3660 - acc: 0.8497 - val_loss: 0.4506 - val_acc: 0.7989\n","Epoch 496/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3782 - acc: 0.8315 - val_loss: 0.4521 - val_acc: 0.7989\n","Epoch 497/1000\n","712/712 [==============================] - 0s 215us/step - loss: 0.3708 - acc: 0.8343 - val_loss: 0.4521 - val_acc: 0.7989\n","Epoch 498/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3606 - acc: 0.8469 - val_loss: 0.4510 - val_acc: 0.7989\n","Epoch 499/1000\n","712/712 [==============================] - 0s 195us/step - loss: 0.3456 - acc: 0.8483 - val_loss: 0.4509 - val_acc: 0.7989\n","Epoch 500/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3597 - acc: 0.8525 - val_loss: 0.4498 - val_acc: 0.7989\n","Epoch 501/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3529 - acc: 0.8539 - val_loss: 0.4482 - val_acc: 0.8045\n","Epoch 502/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3714 - acc: 0.8385 - val_loss: 0.4477 - val_acc: 0.7933\n","Epoch 503/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3667 - acc: 0.8455 - val_loss: 0.4479 - val_acc: 0.7933\n","Epoch 504/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.3551 - acc: 0.8469 - val_loss: 0.4484 - val_acc: 0.7933\n","Epoch 505/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3608 - acc: 0.8441 - val_loss: 0.4491 - val_acc: 0.7933\n","Epoch 506/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3601 - acc: 0.8469 - val_loss: 0.4495 - val_acc: 0.7877\n","Epoch 507/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3572 - acc: 0.8385 - val_loss: 0.4483 - val_acc: 0.7877\n","Epoch 508/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3659 - acc: 0.8455 - val_loss: 0.4479 - val_acc: 0.7877\n","Epoch 509/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3647 - acc: 0.8525 - val_loss: 0.4494 - val_acc: 0.7989\n","Epoch 510/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3645 - acc: 0.8357 - val_loss: 0.4522 - val_acc: 0.7989\n","Epoch 511/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.3612 - acc: 0.8483 - val_loss: 0.4554 - val_acc: 0.7989\n","Epoch 512/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3567 - acc: 0.8357 - val_loss: 0.4548 - val_acc: 0.7989\n","Epoch 513/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3589 - acc: 0.8511 - val_loss: 0.4548 - val_acc: 0.7989\n","Epoch 514/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3529 - acc: 0.8469 - val_loss: 0.4535 - val_acc: 0.7821\n","Epoch 515/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3543 - acc: 0.8441 - val_loss: 0.4530 - val_acc: 0.7821\n","Epoch 516/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3669 - acc: 0.8413 - val_loss: 0.4546 - val_acc: 0.7821\n","Epoch 517/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3650 - acc: 0.8329 - val_loss: 0.4516 - val_acc: 0.7821\n","Epoch 518/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.3573 - acc: 0.8483 - val_loss: 0.4528 - val_acc: 0.7933\n","Epoch 519/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3551 - acc: 0.8441 - val_loss: 0.4513 - val_acc: 0.7933\n","Epoch 520/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3700 - acc: 0.8315 - val_loss: 0.4500 - val_acc: 0.7933\n","Epoch 521/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.3676 - acc: 0.8497 - val_loss: 0.4494 - val_acc: 0.7989\n","Epoch 522/1000\n","712/712 [==============================] - 0s 195us/step - loss: 0.3559 - acc: 0.8371 - val_loss: 0.4491 - val_acc: 0.7989\n","Epoch 523/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3676 - acc: 0.8525 - val_loss: 0.4488 - val_acc: 0.7989\n","Epoch 524/1000\n","712/712 [==============================] - 0s 199us/step - loss: 0.3656 - acc: 0.8539 - val_loss: 0.4493 - val_acc: 0.7933\n","Epoch 525/1000\n","712/712 [==============================] - 0s 223us/step - loss: 0.3626 - acc: 0.8399 - val_loss: 0.4482 - val_acc: 0.7989\n","Epoch 526/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3511 - acc: 0.8329 - val_loss: 0.4469 - val_acc: 0.7933\n","Epoch 527/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3762 - acc: 0.8329 - val_loss: 0.4457 - val_acc: 0.7877\n","Epoch 528/1000\n","712/712 [==============================] - 0s 214us/step - loss: 0.3621 - acc: 0.8287 - val_loss: 0.4441 - val_acc: 0.7989\n","Epoch 529/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3605 - acc: 0.8455 - val_loss: 0.4460 - val_acc: 0.7933\n","Epoch 530/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3645 - acc: 0.8455 - val_loss: 0.4461 - val_acc: 0.7933\n","Epoch 531/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3622 - acc: 0.8497 - val_loss: 0.4437 - val_acc: 0.7933\n","Epoch 532/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3597 - acc: 0.8483 - val_loss: 0.4434 - val_acc: 0.7933\n","Epoch 533/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3744 - acc: 0.8455 - val_loss: 0.4457 - val_acc: 0.7877\n","Epoch 534/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3561 - acc: 0.8399 - val_loss: 0.4485 - val_acc: 0.7933\n","Epoch 535/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3474 - acc: 0.8511 - val_loss: 0.4537 - val_acc: 0.7933\n","Epoch 536/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3626 - acc: 0.8399 - val_loss: 0.4560 - val_acc: 0.7933\n","Epoch 537/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3571 - acc: 0.8469 - val_loss: 0.4571 - val_acc: 0.7933\n","Epoch 538/1000\n","712/712 [==============================] - 0s 228us/step - loss: 0.3661 - acc: 0.8413 - val_loss: 0.4568 - val_acc: 0.7989\n","Epoch 539/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3724 - acc: 0.8385 - val_loss: 0.4558 - val_acc: 0.7989\n","Epoch 540/1000\n","712/712 [==============================] - 0s 207us/step - loss: 0.3626 - acc: 0.8483 - val_loss: 0.4541 - val_acc: 0.8045\n","Epoch 541/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3668 - acc: 0.8371 - val_loss: 0.4526 - val_acc: 0.8045\n","Epoch 542/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3782 - acc: 0.8385 - val_loss: 0.4540 - val_acc: 0.8045\n","Epoch 543/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3708 - acc: 0.8371 - val_loss: 0.4559 - val_acc: 0.8045\n","Epoch 544/1000\n","712/712 [==============================] - 0s 196us/step - loss: 0.3546 - acc: 0.8427 - val_loss: 0.4554 - val_acc: 0.7989\n","Epoch 545/1000\n","712/712 [==============================] - 0s 212us/step - loss: 0.3529 - acc: 0.8469 - val_loss: 0.4545 - val_acc: 0.7989\n","Epoch 546/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3613 - acc: 0.8343 - val_loss: 0.4519 - val_acc: 0.7989\n","Epoch 547/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3498 - acc: 0.8525 - val_loss: 0.4510 - val_acc: 0.8045\n","Epoch 548/1000\n","712/712 [==============================] - 0s 217us/step - loss: 0.3578 - acc: 0.8455 - val_loss: 0.4503 - val_acc: 0.8045\n","Epoch 549/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3459 - acc: 0.8553 - val_loss: 0.4520 - val_acc: 0.8045\n","Epoch 550/1000\n","712/712 [==============================] - 0s 209us/step - loss: 0.3638 - acc: 0.8469 - val_loss: 0.4529 - val_acc: 0.8045\n","Epoch 551/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3758 - acc: 0.8427 - val_loss: 0.4521 - val_acc: 0.8101\n","Epoch 552/1000\n","712/712 [==============================] - 0s 216us/step - loss: 0.3554 - acc: 0.8525 - val_loss: 0.4525 - val_acc: 0.8101\n","Epoch 553/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3553 - acc: 0.8399 - val_loss: 0.4518 - val_acc: 0.8101\n","Epoch 554/1000\n","712/712 [==============================] - 0s 203us/step - loss: 0.3465 - acc: 0.8539 - val_loss: 0.4498 - val_acc: 0.8101\n","Epoch 555/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3617 - acc: 0.8455 - val_loss: 0.4481 - val_acc: 0.8101\n","Epoch 556/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3499 - acc: 0.8469 - val_loss: 0.4469 - val_acc: 0.8045\n","Epoch 557/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3586 - acc: 0.8441 - val_loss: 0.4461 - val_acc: 0.8045\n","Epoch 558/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3737 - acc: 0.8343 - val_loss: 0.4457 - val_acc: 0.8101\n","Epoch 559/1000\n","712/712 [==============================] - 0s 219us/step - loss: 0.3595 - acc: 0.8483 - val_loss: 0.4472 - val_acc: 0.8101\n","Epoch 560/1000\n","712/712 [==============================] - 0s 204us/step - loss: 0.3530 - acc: 0.8497 - val_loss: 0.4473 - val_acc: 0.8045\n","Epoch 561/1000\n","712/712 [==============================] - 0s 205us/step - loss: 0.3585 - acc: 0.8469 - val_loss: 0.4468 - val_acc: 0.8045\n","Epoch 562/1000\n","712/712 [==============================] - 0s 218us/step - loss: 0.3607 - acc: 0.8427 - val_loss: 0.4472 - val_acc: 0.7933\n","Epoch 563/1000\n","712/712 [==============================] - 0s 206us/step - loss: 0.3542 - acc: 0.8441 - val_loss: 0.4470 - val_acc: 0.7933\n","Epoch 564/1000\n","712/712 [==============================] - 0s 210us/step - loss: 0.3643 - acc: 0.8371 - val_loss: 0.4460 - val_acc: 0.7933\n","Epoch 565/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3695 - acc: 0.8427 - val_loss: 0.4459 - val_acc: 0.8101\n","Epoch 566/1000\n","712/712 [==============================] - 0s 220us/step - loss: 0.3438 - acc: 0.8553 - val_loss: 0.4432 - val_acc: 0.8101\n","Epoch 567/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3445 - acc: 0.8553 - val_loss: 0.4439 - val_acc: 0.8045\n","Epoch 568/1000\n","712/712 [==============================] - 0s 198us/step - loss: 0.3523 - acc: 0.8483 - val_loss: 0.4445 - val_acc: 0.8045\n","Epoch 569/1000\n","712/712 [==============================] - 0s 197us/step - loss: 0.3610 - acc: 0.8399 - val_loss: 0.4449 - val_acc: 0.7989\n","Epoch 570/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3559 - acc: 0.8483 - val_loss: 0.4448 - val_acc: 0.7989\n","Epoch 571/1000\n","712/712 [==============================] - 0s 221us/step - loss: 0.3595 - acc: 0.8511 - val_loss: 0.4433 - val_acc: 0.7989\n","Epoch 572/1000\n","712/712 [==============================] - 0s 202us/step - loss: 0.3545 - acc: 0.8427 - val_loss: 0.4436 - val_acc: 0.7989\n","Epoch 573/1000\n","712/712 [==============================] - 0s 234us/step - loss: 0.3627 - acc: 0.8525 - val_loss: 0.4448 - val_acc: 0.7989\n","Epoch 574/1000\n","712/712 [==============================] - 0s 200us/step - loss: 0.3579 - acc: 0.8385 - val_loss: 0.4457 - val_acc: 0.7989\n","Epoch 575/1000\n","712/712 [==============================] - 0s 208us/step - loss: 0.3584 - acc: 0.8483 - val_loss: 0.4441 - val_acc: 0.7989\n","Epoch 576/1000\n","712/712 [==============================] - 0s 201us/step - loss: 0.3615 - acc: 0.8427 - val_loss: 0.4438 - val_acc: 0.7989\n","Epoch 00576: early stopping\n","Training 끝!!\n"],"name":"stdout"}]},{"metadata":{"id":"NrILt_GfoUnh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183},"outputId":"26e07f0d-541f-431d-f7ec-7ac817f50504","executionInfo":{"status":"ok","timestamp":1549521046174,"user_tz":-540,"elapsed":676,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}}},"cell_type":"code","source":["random_forest.fit(rf_X_train, rf_Y_train)\n","predicted = random_forest.predict(X_test)\n","rf_sub_data=np.concatenate((p_id,result),axis=1)\n","rf_sub_data=pd.DataFrame(rf_sub_data,columns=['PassengerId','Survived'])\n","print(sub_data.head())\n","rf_sub_data.to_csv('/content/gdrive/My Drive/submission_rf2.csv',index=False)"],"execution_count":274,"outputs":[{"output_type":"stream","text":["   PassengerId  Survived\n","0          892         0\n","1          893         0\n","2          894         0\n","3          895         0\n","4          896         1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]},{"metadata":{"id":"9HMlzza0vCn3","colab_type":"code","outputId":"0c181002-c227-43ee-d8bf-65677e699e2f","executionInfo":{"status":"ok","timestamp":1549519827501,"user_tz":-540,"elapsed":21458,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"cell_type":"code","source":["#Y_test = model.predict_classes(X_test)\n","predictions = []                                       # For ensemble averaging   \n","for i in range(ensemble_size):\n","    temp = models[i].predict_classes(X_test)\n","    predictions.append(temp)\n","result = sum(predictions)\n","print(result[0:5])\n","np.place(result, result<=(ensemble_size/2), 0)\n","np.place(result, result>(ensemble_size/2), 1)\n","print(result[0:5])"],"execution_count":266,"outputs":[{"output_type":"stream","text":["[[0]\n"," [0]\n"," [0]\n"," [0]\n"," [1]]\n","[[0]\n"," [0]\n"," [0]\n"," [0]\n"," [1]]\n"],"name":"stdout"}]},{"metadata":{"id":"ySehPdLK-KRq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"outputId":"9a952f09-ed3b-45d4-955c-dfe4f9c28f94","executionInfo":{"status":"ok","timestamp":1549519833552,"user_tz":-540,"elapsed":655,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}}},"cell_type":"code","source":["# 제출 파일 만들기\n","#Y_test = np.greater(Y_test,0.5).astype(int)\n","sub_data=np.concatenate((p_id,result),axis=1)\n","sub_data=pd.DataFrame(sub_data,columns=['PassengerId','Survived'])\n","print(sub_data.head())\n","sub_data.to_csv('/content/gdrive/My Drive/submission3.csv',index=False)"],"execution_count":267,"outputs":[{"output_type":"stream","text":["   PassengerId  Survived\n","0          892         0\n","1          893         0\n","2          894         0\n","3          895         0\n","4          896         1\n"],"name":"stdout"}]},{"metadata":{"id":"6XouGrmnyAEH","colab_type":"code","outputId":"342cfd0e-ce12-4932-ec3a-af16a4749b3a","executionInfo":{"status":"ok","timestamp":1549517231055,"user_tz":-540,"elapsed":898,"user":{"displayName":"박승화","photoUrl":"https://lh4.googleusercontent.com/-Yw90gGef6jU/AAAAAAAAAAI/AAAAAAAAABo/Z3qGqQnBTjo/s64/photo.jpg","userId":"14571056521829072048"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"cell_type":"code","source":["grouped = data_train.groupby(['Sex','Pclass', 'Initial'])\n","grouped.Age.median()\n","#data_train.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))"],"execution_count":229,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sex  Pclass  Initial\n","0    1       0           4.0\n","             2          36.0\n","             4          56.0\n","     2       0           1.0\n","             2          32.0\n","             4          46.5\n","     3       0           5.0\n","             2          32.0\n","1    1       1          29.0\n","             2          49.0\n","             3          38.5\n","     2       1          24.0\n","             3          32.0\n","     3       1          22.0\n","             3          36.0\n","Name: Age, dtype: float64"]},"metadata":{"tags":[]},"execution_count":229}]},{"metadata":{"id":"x76LtxmeyTmS","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gxxM6WLG17LZ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}