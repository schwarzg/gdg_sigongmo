{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (8000, 784)\n",
      "Y train shape:  (8000, 10)\n",
      "y shape:  (10000,)\n",
      "Y shape:  (10000, 10)\n",
      "---------------------Sigmoid function---------------------\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 2.2522 - acc: 0.1921\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 2.0387 - acc: 0.4409\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 1.6070 - acc: 0.6263\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.1545 - acc: 0.7255\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.8781 - acc: 0.7924\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.7085 - acc: 0.8336\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.6086 - acc: 0.8545\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.5454 - acc: 0.8680\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.5041 - acc: 0.8741\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4664 - acc: 0.8777\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4366 - acc: 0.8862\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.4208 - acc: 0.8922\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4052 - acc: 0.8959\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.4045 - acc: 0.8956\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.3936 - acc: 0.8975\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.3822 - acc: 0.8967\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.3821 - acc: 0.8986\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.3763 - acc: 0.8975\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3691 - acc: 0.8979\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.3609 - acc: 0.9006\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3569 - acc: 0.9021\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3737 - acc: 0.8970\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3607 - acc: 0.9027\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.3685 - acc: 0.8976\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3652 - acc: 0.8982\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3595 - acc: 0.8976\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.3635 - acc: 0.8972\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3850 - acc: 0.8907\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.3588 - acc: 0.8996\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.3729 - acc: 0.8949\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3637 - acc: 0.8969\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.3851 - acc: 0.8882\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.3703 - acc: 0.8917\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.3548 - acc: 0.8941\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3546 - acc: 0.8991\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3494 - acc: 0.8959\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3566 - acc: 0.8960\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3671 - acc: 0.8912\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3626 - acc: 0.8900\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3834 - acc: 0.8849\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3831 - acc: 0.8857\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3734 - acc: 0.8886\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3878 - acc: 0.8826\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.3927 - acc: 0.8782\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.3935 - acc: 0.8794\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.4085 - acc: 0.8721\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.4157 - acc: 0.8734\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.3893 - acc: 0.8839\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4234 - acc: 0.8762\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4080 - acc: 0.8739\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.4320 - acc: 0.8674\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.4354 - acc: 0.8652\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4379 - acc: 0.8679\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4068 - acc: 0.8791\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4381 - acc: 0.8661\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.4259 - acc: 0.8742\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4313 - acc: 0.8675\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4006 - acc: 0.8766\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.4054 - acc: 0.8701\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.4171 - acc: 0.8686\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.3914 - acc: 0.8790\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.4046 - acc: 0.8705\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.4193 - acc: 0.8709\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.4181 - acc: 0.8695\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 0.4227 - acc: 0.8661\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.4203 - acc: 0.8716\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 0.4001 - acc: 0.8775\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.4058 - acc: 0.8777\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.4044 - acc: 0.8752\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 0.4026 - acc: 0.8714\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 0.3972 - acc: 0.8784\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.4627 - acc: 0.8535\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 0.4366 - acc: 0.8660\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.4049 - acc: 0.8752\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 149us/step - loss: 0.4296 - acc: 0.8667\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.4337 - acc: 0.8661\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.4672 - acc: 0.8515\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.4628 - acc: 0.8575\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.4688 - acc: 0.8517\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4204 - acc: 0.8697\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.4321 - acc: 0.8657\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.4232 - acc: 0.8680\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4116 - acc: 0.8724\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4194 - acc: 0.8680\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4015 - acc: 0.8690\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.4063 - acc: 0.8714\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4367 - acc: 0.8591\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4318 - acc: 0.8605\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 0.4004 - acc: 0.8769\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4006 - acc: 0.8730\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.3965 - acc: 0.8754\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.4379 - acc: 0.8636\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.4602 - acc: 0.8482\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.4376 - acc: 0.8616\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.4278 - acc: 0.8601\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.4137 - acc: 0.8709\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.4231 - acc: 0.8665\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.4363 - acc: 0.8599\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.4337 - acc: 0.8627\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.4266 - acc: 0.8642\n",
      "----------------------------------------------------------\n",
      "---------------------Tanh function---------------------\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 1.1117 - acc: 0.6424\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 0.9538 - acc: 0.6788\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.9111 - acc: 0.6865\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 1.0116 - acc: 0.6488\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 1.0847 - acc: 0.6204\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 1.0629 - acc: 0.6346\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 0.9991 - acc: 0.6556\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 1.0210 - acc: 0.6535\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.9321 - acc: 0.6886\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.8853 - acc: 0.6983\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 0.8464 - acc: 0.7118\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.8941 - acc: 0.6915\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.9531 - acc: 0.6680\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.8672 - acc: 0.6966\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.8650 - acc: 0.7030\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.8556 - acc: 0.7088\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.8775 - acc: 0.6925\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.8718 - acc: 0.7011\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.8813 - acc: 0.7058\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.8529 - acc: 0.7088\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.8318 - acc: 0.7165\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.9864 - acc: 0.6698\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.9409 - acc: 0.6715\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.8629 - acc: 0.7055\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.8752 - acc: 0.6923\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.9672 - acc: 0.6535\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.9582 - acc: 0.6541\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.9128 - acc: 0.6785\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.8219 - acc: 0.7180\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.8235 - acc: 0.7219\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.8218 - acc: 0.7124\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 0.8523 - acc: 0.7014\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.8455 - acc: 0.7124\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.8594 - acc: 0.7066\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.8869 - acc: 0.6954\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.9907 - acc: 0.6606\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.9920 - acc: 0.6604\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 1.0304 - acc: 0.6414\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 1.0992 - acc: 0.6215\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 1.0812 - acc: 0.6121\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 1.0046 - acc: 0.6318\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.9939 - acc: 0.6475\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.9589 - acc: 0.6625\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.9527 - acc: 0.6529\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.9956 - acc: 0.6279\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 0.9502 - acc: 0.6529\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 0.9546 - acc: 0.6430\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 0.9295 - acc: 0.6456\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.9492 - acc: 0.6284\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 0.9336 - acc: 0.6494\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 149us/step - loss: 0.9197 - acc: 0.6531\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.9172 - acc: 0.6548\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.9284 - acc: 0.6474\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 0.9002 - acc: 0.6746\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.9715 - acc: 0.6465\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.9648 - acc: 0.6564\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.9368 - acc: 0.6734\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 0.9838 - acc: 0.6358\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.9532 - acc: 0.6449\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 142us/step - loss: 0.9724 - acc: 0.6480\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 1.0348 - acc: 0.6153\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 1.0900 - acc: 0.5815\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 1.0336 - acc: 0.6118\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 1.0258 - acc: 0.6181\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 1.0318 - acc: 0.6104\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 149us/step - loss: 1.0093 - acc: 0.6268\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.9964 - acc: 0.6390\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.9878 - acc: 0.6424\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0676 - acc: 0.6134\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 1.0874 - acc: 0.6011\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 1.0821 - acc: 0.5998\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 1.0187 - acc: 0.6311\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.9803 - acc: 0.6538\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.9784 - acc: 0.6510\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.9696 - acc: 0.6509\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 0.9433 - acc: 0.6616\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.9170 - acc: 0.6778\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.9878 - acc: 0.6448\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 1.0888 - acc: 0.6213\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 1.1355 - acc: 0.6110\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 1.1316 - acc: 0.6131\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.1495 - acc: 0.5913\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0465 - acc: 0.6485\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0510 - acc: 0.6380\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 1.0314 - acc: 0.6429\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 1.0159 - acc: 0.6501\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 1.0266 - acc: 0.6399\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 1.0441 - acc: 0.6319\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 1.0681 - acc: 0.6309\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 1.0511 - acc: 0.6409\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0995 - acc: 0.6199\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0486 - acc: 0.6413\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0259 - acc: 0.6430\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 1.0305 - acc: 0.6441\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0188 - acc: 0.6418\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 1.0013 - acc: 0.6460\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 1.0089 - acc: 0.6459\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0561 - acc: 0.6330\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 1.0530 - acc: 0.6405\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 1.0316 - acc: 0.6369\n",
      "----------------------------------------------------------\n",
      "---------------------ReLU function---------------------\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7389 - acc: 0.0855\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 14.7460 - acc: 0.0851\n",
      "----------------------------------------------------------\n",
      "---------------------LeakyReLU function---------------------\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 14.4992 - acc: 0.1004\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.5083 - acc: 0.0999\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.5083 - acc: 0.0999\n",
      "----------------------------------------------------------\n",
      "---------------------PReLU function---------------------\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 14.7438 - acc: 0.0851\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 173us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 178us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 172us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 179us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 177us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 167us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 163us/step - loss: 14.7460 - acc: 0.0851\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 14.7460 - acc: 0.0851\n",
      "----------------------------------------------------------\n",
      "---------------------Prediction accuracy Evalutation---------------------\n",
      "2000/2000 [==============================] - 0s 48us/step\n",
      "2000/2000 [==============================] - 0s 47us/step\n",
      "2000/2000 [==============================] - 0s 48us/step\n",
      "2000/2000 [==============================] - 0s 49us/step\n",
      "2000/2000 [==============================] - 0s 23us/step\n",
      "sigmoid:  [0.515908836364746, 0.837]\n",
      "tanh:  [1.1014872465133667, 0.62]\n",
      "ReLU:  [14.804470382690429, 0.0815]\n",
      "LeakyReLU:  [14.514344665527343, 0.0995]\n",
      "PReLU:  [14.804470382690429, 0.0815]\n"
     ]
    }
   ],
   "source": [
    "# CH4. MNIST\n",
    "# ACtivation Function: sigmoid, tanh, ReLU, LeakyReLU, PReLU\n",
    "from sklearn import datasets\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# MNIST 다운로드\n",
    "mnist1 = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "\n",
    "# 에러가 발생하여 https://github.com/wikibook/deep-learning-with-tensorflow 에서 다운로드 후 mldata 폴더에 삽입함\n",
    "# MNIST 파일 (mnist-original.mat) 읽기\n",
    "mnist = scipy.io.loadmat('./mldata/mnist-original.mat')\n",
    "mnist_data = mnist['data']\n",
    "mnist_label = mnist['label']\n",
    "mnist_data = mnist_data.T\n",
    "mnist_label = mnist_label\n",
    "\n",
    "n=len(mnist1.data) #n = len(mnist_data)\n",
    "N = 10000 # MNIST의 부분적인 데이터로 실험\n",
    "indices = np.random.permutation(range(n))[:N] # 무작위로 N장을 선택한다.\n",
    "X=mnist1.data[indices]#X = mnist_data[indices]\n",
    "y=mnist1.target[indices]#y = mnist_label[:,indices]\n",
    "Y=np.eye(10)[y.astype(int)] #Y = np.eye(10)[y.astype(int)] # 1-of-K 표현으로 변환 \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "print('X train shape: ', X_train.shape)\n",
    "print('Y train shape: ', Y_train.shape)\n",
    "print('y shape: ', y.shape)\n",
    "print('Y shape: ', Y.shape)\n",
    "#print(y)\n",
    "#print()\n",
    "#print(Y)\n",
    "# Model 생성\n",
    "n_in = len(X[0]) # 784 (32*32)\n",
    "n_hidden = 200\n",
    "n_out = len(Y[0]) # 10\n",
    "\n",
    "# ACtivation function: Sigmoid\n",
    "model_sigmoid = Sequential()\n",
    "model_sigmoid .add(Dense(n_hidden, input_dim=n_in))\n",
    "model_sigmoid .add(Activation('sigmoid'))\n",
    "\n",
    "model_sigmoid .add(Dense(n_hidden))\n",
    "model_sigmoid .add(Activation('sigmoid'))\n",
    "\n",
    "model_sigmoid .add(Dense(n_hidden))\n",
    "model_sigmoid .add(Activation('sigmoid'))\n",
    "\n",
    "model_sigmoid .add(Dense(n_out))\n",
    "model_sigmoid .add(Activation('softmax')) # multi-class\n",
    "\n",
    "model_sigmoid .compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "# ACtivation function: tanh function -> sigmoid의 도함수의 최댓값이 0.25이다. tanh의 도함수의 최댓값은 1(sigmoid을 닮았고 경로 손실x)\n",
    "model_tanh = Sequential()\n",
    "model_tanh .add(Dense(n_hidden, input_dim=n_in))\n",
    "model_tanh .add(Activation('tanh'))\n",
    "\n",
    "model_tanh .add(Dense(n_hidden))\n",
    "model_tanh .add(Activation('tanh'))\n",
    "\n",
    "model_tanh .add(Dense(n_hidden))\n",
    "model_tanh .add(Activation('tanh'))\n",
    "\n",
    "model_tanh .add(Dense(n_out))\n",
    "model_tanh .add(Activation('softmax')) # multi-class\n",
    "\n",
    "model_tanh .compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "# ACtivation function: ReLU -> tanh도 마찬가지로 고차원 데이터를 다룰 경우 경사가 소실되는 문제가 발생, 지수함수를 포함하지 않고\n",
    "#                           단순한 수식으로 표현되어 빠르게 계산됨\n",
    "model_ReLU = Sequential()\n",
    "model_ReLU .add(Dense(n_hidden, input_dim=n_in))\n",
    "model_ReLU .add(Activation('relu'))\n",
    "\n",
    "model_ReLU .add(Dense(n_hidden))\n",
    "model_ReLU .add(Activation('relu'))\n",
    "\n",
    "model_ReLU .add(Dense(n_hidden))\n",
    "model_ReLU .add(Activation('relu'))\n",
    "\n",
    "model_ReLU .add(Dense(n_out))\n",
    "model_ReLU .add(Activation('softmax')) # multi-class\n",
    "\n",
    "model_ReLU .compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "# ACtivation function: LeaklyReLU -> ReLU에서 x<0일 때 경사가 사라져버려서 학습 과정이 불안정해짐\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "alpha = 0.01 # f(x)=max(ax, x)\n",
    "model_LeakyReLU = Sequential()\n",
    "model_LeakyReLU .add(Dense(n_hidden, input_dim=n_in))\n",
    "model_LeakyReLU .add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "model_LeakyReLU .add(Dense(n_hidden))\n",
    "model_LeakyReLU .add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "model_LeakyReLU .add(Dense(n_hidden))\n",
    "model_LeakyReLU .add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "model_LeakyReLU .add(Dense(n_out))\n",
    "model_LeakyReLU .add(Activation('softmax')) # multi-class\n",
    "\n",
    "model_LeakyReLU .compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "# Activation function: Parametric ReLU -> LeakyReLU에서 x<0일 때 경사가 a로 고정되어 있지만 이 부분도 학습을 하자\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "model_PReLU = Sequential()\n",
    "model_PReLU .add(Dense(n_hidden, input_dim=n_in))\n",
    "model_PReLU .add(PReLU())\n",
    "\n",
    "model_PReLU .add(Dense(n_hidden))\n",
    "model_PReLU .add(PReLU())\n",
    "\n",
    "model_PReLU .add(Dense(n_hidden))\n",
    "model_PReLU .add(PReLU())\n",
    "\n",
    "model_PReLU .add(Dense(n_out))\n",
    "model_PReLU .add(Activation('softmax')) # multi-class\n",
    "\n",
    "model_PReLU .compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 학습\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "print('---------------------Sigmoid function---------------------')\n",
    "model_sigmoid .fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "print('----------------------------------------------------------')\n",
    "print('---------------------Tanh function---------------------')\n",
    "model_tanh .fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "print('----------------------------------------------------------')\n",
    "print('---------------------ReLU function---------------------')\n",
    "model_ReLU.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "print('----------------------------------------------------------')\n",
    "print('---------------------LeakyReLU function---------------------')\n",
    "model_LeakyReLU.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "print('----------------------------------------------------------')\n",
    "print('---------------------PReLU function---------------------')\n",
    "model_PReLU.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size)\n",
    "print('----------------------------------------------------------')\n",
    "# 예측 정확도 평가\n",
    "print('---------------------Prediction accuracy Evalutation---------------------')\n",
    "loss_and_metrics_sigmoid = model_sigmoid.evaluate(X_test, Y_test)\n",
    "loss_and_metrics_tanh = model_tanh.evaluate(X_test, Y_test)\n",
    "loss_and_metrics_ReLU = model_ReLU.evaluate(X_test, Y_test)\n",
    "loss_and_metrics_LeakyReLU = model_LeakyReLU.evaluate(X_test, Y_test)\n",
    "loss_and_metrics_PReLU = model_ReLU.evaluate(X_test, Y_test)\n",
    "print('sigmoid: ', loss_and_metrics_sigmoid)\n",
    "print('tanh: ', loss_and_metrics_tanh)\n",
    "print('ReLU: ', loss_and_metrics_ReLU)\n",
    "print('LeakyReLU: ', loss_and_metrics_LeakyReLU)\n",
    "print('PReLU: ', loss_and_metrics_PReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------ReLU & Dropout---------------------\n",
      "Train on 1600 samples, validate on 6400 samples\n",
      "Epoch 1/50\n",
      "1600/1600 [==============================] - 1s 447us/step - loss: 2.2994 - acc: 0.1175 - val_loss: 2.2932 - val_acc: 0.2036\n",
      "Epoch 2/50\n",
      "1600/1600 [==============================] - 0s 104us/step - loss: 2.2881 - acc: 0.1488 - val_loss: 2.2765 - val_acc: 0.1453\n",
      "Epoch 3/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 2.2699 - acc: 0.1500 - val_loss: 2.2388 - val_acc: 0.1208\n",
      "Epoch 4/50\n",
      "1600/1600 [==============================] - 0s 95us/step - loss: 2.2308 - acc: 0.1406 - val_loss: 2.1583 - val_acc: 0.2123\n",
      "Epoch 5/50\n",
      "1600/1600 [==============================] - 0s 95us/step - loss: 2.1377 - acc: 0.1969 - val_loss: 2.0064 - val_acc: 0.4239\n",
      "Epoch 6/50\n",
      "1600/1600 [==============================] - 0s 94us/step - loss: 1.9771 - acc: 0.3100 - val_loss: 1.7203 - val_acc: 0.5009\n",
      "Epoch 7/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 1.6924 - acc: 0.4162 - val_loss: 1.3129 - val_acc: 0.6064\n",
      "Epoch 8/50\n",
      "1600/1600 [==============================] - 0s 88us/step - loss: 1.3812 - acc: 0.5119 - val_loss: 1.0318 - val_acc: 0.6825\n",
      "Epoch 9/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 1.2035 - acc: 0.5669 - val_loss: 0.9337 - val_acc: 0.6948\n",
      "Epoch 10/50\n",
      "1600/1600 [==============================] - 0s 91us/step - loss: 1.0817 - acc: 0.6100 - val_loss: 0.8191 - val_acc: 0.7345\n",
      "Epoch 11/50\n",
      "1600/1600 [==============================] - 0s 89us/step - loss: 0.9815 - acc: 0.6519 - val_loss: 0.7278 - val_acc: 0.7700\n",
      "Epoch 12/50\n",
      "1600/1600 [==============================] - 0s 94us/step - loss: 0.8912 - acc: 0.6969 - val_loss: 0.6709 - val_acc: 0.7859\n",
      "Epoch 13/50\n",
      "1600/1600 [==============================] - 0s 98us/step - loss: 0.8422 - acc: 0.7063 - val_loss: 0.5937 - val_acc: 0.8217\n",
      "Epoch 14/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.7191 - acc: 0.7550 - val_loss: 0.5820 - val_acc: 0.8180\n",
      "Epoch 15/50\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.6898 - acc: 0.7663 - val_loss: 0.5392 - val_acc: 0.8373\n",
      "Epoch 16/50\n",
      "1600/1600 [==============================] - 0s 93us/step - loss: 0.6625 - acc: 0.7731 - val_loss: 0.5705 - val_acc: 0.8097\n",
      "Epoch 17/50\n",
      "1600/1600 [==============================] - 0s 105us/step - loss: 0.5753 - acc: 0.8063 - val_loss: 0.4752 - val_acc: 0.8589\n",
      "Epoch 18/50\n",
      "1600/1600 [==============================] - 0s 112us/step - loss: 0.5792 - acc: 0.8081 - val_loss: 0.4640 - val_acc: 0.8613\n",
      "Epoch 19/50\n",
      "1600/1600 [==============================] - 0s 94us/step - loss: 0.5165 - acc: 0.8356 - val_loss: 0.4446 - val_acc: 0.8644\n",
      "Epoch 20/50\n",
      "1600/1600 [==============================] - 0s 89us/step - loss: 0.5232 - acc: 0.8269 - val_loss: 0.4428 - val_acc: 0.8652\n",
      "Epoch 21/50\n",
      "1600/1600 [==============================] - 0s 87us/step - loss: 0.4588 - acc: 0.8544 - val_loss: 0.4060 - val_acc: 0.8792\n",
      "Epoch 22/50\n",
      "1600/1600 [==============================] - 0s 87us/step - loss: 0.4275 - acc: 0.8656 - val_loss: 0.4073 - val_acc: 0.8778\n",
      "Epoch 23/50\n",
      "1600/1600 [==============================] - 0s 91us/step - loss: 0.3910 - acc: 0.8744 - val_loss: 0.3826 - val_acc: 0.8884\n",
      "Epoch 24/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.3846 - acc: 0.8688 - val_loss: 0.4150 - val_acc: 0.8769\n",
      "Epoch 25/50\n",
      "1600/1600 [==============================] - 0s 88us/step - loss: 0.3607 - acc: 0.8863 - val_loss: 0.3877 - val_acc: 0.8877\n",
      "Epoch 26/50\n",
      "1600/1600 [==============================] - 0s 91us/step - loss: 0.3673 - acc: 0.8794 - val_loss: 0.3687 - val_acc: 0.8930\n",
      "Epoch 27/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.3371 - acc: 0.8969 - val_loss: 0.3700 - val_acc: 0.8934\n",
      "Epoch 28/50\n",
      "1600/1600 [==============================] - 0s 99us/step - loss: 0.3224 - acc: 0.8881 - val_loss: 0.4000 - val_acc: 0.8736\n",
      "Epoch 29/50\n",
      "1600/1600 [==============================] - 0s 100us/step - loss: 0.3116 - acc: 0.9013 - val_loss: 0.3637 - val_acc: 0.8973\n",
      "Epoch 30/50\n",
      "1600/1600 [==============================] - 0s 107us/step - loss: 0.2775 - acc: 0.9219 - val_loss: 0.3813 - val_acc: 0.8898\n",
      "Epoch 31/50\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.2735 - acc: 0.9081 - val_loss: 0.3577 - val_acc: 0.8978\n",
      "Epoch 32/50\n",
      "1600/1600 [==============================] - 0s 103us/step - loss: 0.2621 - acc: 0.9125 - val_loss: 0.3536 - val_acc: 0.9005\n",
      "Epoch 33/50\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.2386 - acc: 0.9175 - val_loss: 0.3894 - val_acc: 0.8864\n",
      "Epoch 34/50\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.2181 - acc: 0.9300 - val_loss: 0.3488 - val_acc: 0.9014\n",
      "Epoch 35/50\n",
      "1600/1600 [==============================] - 0s 94us/step - loss: 0.2390 - acc: 0.9213 - val_loss: 0.3498 - val_acc: 0.9027\n",
      "Epoch 36/50\n",
      "1600/1600 [==============================] - 0s 92us/step - loss: 0.2059 - acc: 0.9356 - val_loss: 0.3523 - val_acc: 0.8958\n",
      "Epoch 37/50\n",
      "1600/1600 [==============================] - 0s 92us/step - loss: 0.1864 - acc: 0.9425 - val_loss: 0.3626 - val_acc: 0.8984\n",
      "Epoch 38/50\n",
      "1600/1600 [==============================] - 0s 89us/step - loss: 0.2095 - acc: 0.9356 - val_loss: 0.3533 - val_acc: 0.9017\n",
      "Epoch 39/50\n",
      "1600/1600 [==============================] - 0s 93us/step - loss: 0.2129 - acc: 0.9281 - val_loss: 0.3577 - val_acc: 0.9039\n",
      "Epoch 40/50\n",
      "1600/1600 [==============================] - 0s 89us/step - loss: 0.1736 - acc: 0.9462 - val_loss: 0.3723 - val_acc: 0.8994\n",
      "Epoch 41/50\n",
      "1600/1600 [==============================] - 0s 92us/step - loss: 0.1633 - acc: 0.9456 - val_loss: 0.3526 - val_acc: 0.9055\n",
      "Epoch 42/50\n",
      "1600/1600 [==============================] - 0s 93us/step - loss: 0.1773 - acc: 0.9431 - val_loss: 0.3485 - val_acc: 0.9034\n",
      "Epoch 43/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.1751 - acc: 0.9425 - val_loss: 0.3627 - val_acc: 0.9034\n",
      "Epoch 44/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.1514 - acc: 0.9544 - val_loss: 0.3630 - val_acc: 0.9011\n",
      "Epoch 45/50\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.1428 - acc: 0.9556 - val_loss: 0.3587 - val_acc: 0.9059\n",
      "Epoch 46/50\n",
      "1600/1600 [==============================] - 0s 93us/step - loss: 0.1525 - acc: 0.9531 - val_loss: 0.3407 - val_acc: 0.9105\n",
      "Epoch 47/50\n",
      "1600/1600 [==============================] - 0s 92us/step - loss: 0.1275 - acc: 0.9594 - val_loss: 0.3604 - val_acc: 0.9072\n",
      "Epoch 48/50\n",
      "1600/1600 [==============================] - 0s 94us/step - loss: 0.1148 - acc: 0.9613 - val_loss: 0.3447 - val_acc: 0.9141\n",
      "Epoch 49/50\n",
      "1600/1600 [==============================] - 0s 93us/step - loss: 0.1278 - acc: 0.9581 - val_loss: 0.3402 - val_acc: 0.9133\n",
      "Epoch 50/50\n",
      "1600/1600 [==============================] - 0s 92us/step - loss: 0.1498 - acc: 0.9556 - val_loss: 0.3497 - val_acc: 0.9106\n",
      "----------------------------------------------------------\n",
      "2000/2000 [==============================] - 0s 30us/step\n",
      "ReLU & Dropout:  [0.30621730956435206, 0.924]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropout\n",
    "n_in = len(X[0])  # 784\n",
    "n_hiddens = [200, 200, 200]\n",
    "n_out = len(Y[0])  # 10\n",
    "dropout = 0.5\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "# 모델 생성\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as K\n",
    "def weight_variable(shape):\n",
    "    return K.truncated_normal(shape, stddev=0.01)\n",
    "    # return np.random.normal(scale=0.01, size=shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim,\n",
    "                    kernel_initializer=weight_variable))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Validation\n",
    "N_train = 0.8\n",
    "N_validation = 0.2\n",
    "# 전체 데이터를 test와 (train, validation) 데이터 셋으로 분류\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=N_train)\n",
    "# train와 validation 데이터 셋으로 분류\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, train_size=N_validation)\n",
    "\n",
    "print('---------------------ReLU & Dropout---------------------')\n",
    "hist = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                validation_data = (X_validation, Y_validation))\n",
    "val_acc = hist.history['val_acc']\n",
    "val_loss = hist.history['val_loss']\n",
    "print('----------------------------------------------------------')\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print('ReLU & Dropout: ', loss_and_metrics)\n",
    "\n",
    "\n",
    "# 평가\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), val_acc, label='acc', color='black')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 평가\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs), val_acc, label='acc', color='black')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
